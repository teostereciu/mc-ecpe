{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 1103\n",
    "\n",
    "notebooks_dir = Path.cwd()\n",
    "project_dir = notebooks_dir.parent\n",
    "data_dir = project_dir / 'data' / 'raw'\n",
    "text_data_path = data_dir / 'Subtask_2_train.json'\n",
    "interim_dir = project_dir / 'data' / 'interim'\n",
    "mel_dir = interim_dir / 'mel' \n",
    "models_dir = project_dir / 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'utterance_ID': 1, 'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .', 'speaker': 'Chandler', 'emotion': 'neutral', 'video_name': 'dia1utt1.mp4'}\n",
      "{'utterance_ID': 2, 'text': 'Oh , yeah . Had that dream .', 'speaker': 'All', 'emotion': 'neutral', 'video_name': 'dia1utt2.mp4'}\n",
      "{'utterance_ID': 3, 'text': 'Then I look down , and I realize there is a phone ... there .', 'speaker': 'Chandler', 'emotion': 'surprise', 'video_name': 'dia1utt3.mp4'}\n",
      "{'utterance_ID': 4, 'text': 'Instead of ... ?', 'speaker': 'Joey', 'emotion': 'surprise', 'video_name': 'dia1utt4.mp4'}\n",
      "{'utterance_ID': 5, 'text': 'That is right .', 'speaker': 'Chandler', 'emotion': 'anger', 'video_name': 'dia1utt5.mp4'}\n",
      "{'utterance_ID': 6, 'text': 'Never had that dream .', 'speaker': 'Joey', 'emotion': 'neutral', 'video_name': 'dia1utt6.mp4'}\n",
      "{'utterance_ID': 7, 'text': 'No .', 'speaker': 'Phoebe', 'emotion': 'neutral', 'video_name': 'dia1utt7.mp4'}\n",
      "{'utterance_ID': 8, 'text': 'All of a sudden , the phone starts to ring .', 'speaker': 'Chandler', 'emotion': 'neutral', 'video_name': 'dia1utt8.mp4'}\n"
     ]
    }
   ],
   "source": [
    "text_data = json.loads(text_data_path.read_text())\n",
    "for i in range(len(text_data[0]['conversation'])):\n",
    "    print(text_data[0]['conversation'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_json(json):\n",
    "    text = []\n",
    "    emotions = []\n",
    "    filenames = []\n",
    "    for line_idx in range(len(json)):\n",
    "        text += [json[line_idx]['text']]\n",
    "        emotions += [json[line_idx]['emotion']]\n",
    "        filenames += [json[line_idx]['video_name'][:-4]]\n",
    "    return text, emotions, filenames\n",
    "\n",
    "convos = []\n",
    "all_emotions = []\n",
    "all_filenames = []\n",
    "for convo_idx in range(len(text_data)):\n",
    "    convo, emotions, filenames = get_data_from_json(text_data[convo_idx]['conversation'])\n",
    "    convos += convo\n",
    "    all_emotions += emotions\n",
    "    all_filenames += filenames\n",
    "\n",
    "data = {'filename': all_filenames, 'text' : convos, 'label': all_emotions}\n",
    "text_data_dict = {'text' : convos, 'label': all_emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alright , so I am back in high school , I am s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh , yeah . Had that dream .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Then I look down , and I realize there is a ph...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Instead of ... ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That is right .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Alright , so I am back in high school , I am s...      0\n",
       "1                       Oh , yeah . Had that dream .      0\n",
       "2  Then I look down , and I realize there is a ph...      2\n",
       "3                                   Instead of ... ?      2\n",
       "4                                    That is right .      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(text_data_dict)\n",
    "\n",
    "labels = ['neutral', 'joy', 'surprise', 'anger', 'fear', 'disgust', 'sadness']\n",
    "label2id = {label: index for index, label in enumerate(labels)}\n",
    "id2label = {index: label for index, label in enumerate(labels)}\n",
    "\n",
    "df['label'] = df['label'].apply(lambda label: label2id[label])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.distilbert.tokenization_distilbert because of the following error (look up to see its traceback):\ncannot import name '__version__' from 'tokenizers.tokenizers' (/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tokenizers/tokenizers.cpython-310-darwin.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union, overload\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m     28\u001b[0m     ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,\n\u001b[1;32m     29\u001b[0m     INIT_TOKENIZER_DOCSTRING,\n\u001b[1;32m     30\u001b[0m     AddedToken,\n\u001b[1;32m     31\u001b[0m     BatchEncoding,\n\u001b[1;32m     32\u001b[0m     EncodedInput,\n\u001b[1;32m     33\u001b[0m     EncodedInputPair,\n\u001b[1;32m     34\u001b[0m     PreTokenizedInput,\n\u001b[1;32m     35\u001b[0m     PreTokenizedInputPair,\n\u001b[1;32m     36\u001b[0m     PreTrainedTokenizerBase,\n\u001b[1;32m     37\u001b[0m     TextInput,\n\u001b[1;32m     38\u001b[0m     TextInputPair,\n\u001b[1;32m     39\u001b[0m     TruncationStrategy,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaddingStrategy, TensorType, add_end_docstrings, logging\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:77\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AddedToken\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoding \u001b[38;5;28;01mas\u001b[39;00m EncodingFast\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tokenizers/__init__.py:78\u001b[0m\n\u001b[1;32m     75\u001b[0m     CONTIGUOUS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontiguous\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     79\u001b[0m     AddedToken,\n\u001b[1;32m     80\u001b[0m     Encoding,\n\u001b[1;32m     81\u001b[0m     NormalizedString,\n\u001b[1;32m     82\u001b[0m     PreTokenizedString,\n\u001b[1;32m     83\u001b[0m     Regex,\n\u001b[1;32m     84\u001b[0m     Token,\n\u001b[1;32m     85\u001b[0m     Tokenizer,\n\u001b[1;32m     86\u001b[0m     decoders,\n\u001b[1;32m     87\u001b[0m     models,\n\u001b[1;32m     88\u001b[0m     normalizers,\n\u001b[1;32m     89\u001b[0m     pre_tokenizers,\n\u001b[1;32m     90\u001b[0m     processors,\n\u001b[1;32m     91\u001b[0m     trainers,\n\u001b[1;32m     92\u001b[0m     __version__,\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     BertWordPieceTokenizer,\n\u001b[1;32m     96\u001b[0m     ByteLevelBPETokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     SentencePieceUnigramTokenizer,\n\u001b[1;32m    100\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '__version__' from 'tokenizers.tokenizers' (/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tokenizers/tokenizers.cpython-310-darwin.so)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDistilBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1090\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1082\u001b[0m PYTORCH_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\u001b[39m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124minstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\u001b[39m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124mPlease note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[0;32m-> 1090\u001b[0m TORCHVISION_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the Torchvision library but it was not found in your environment. Checkout the instructions on the\u001b[39m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;124minstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\u001b[39m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124mPlease note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;66;03m# docstyle-ignore\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m PYTORCH_IMPORT_ERROR_WITH_TF \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the PyTorch library but it was not found in your environment.\u001b[39m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124mHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124mmatch your environment.\u001b[39m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1089\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.distilbert.tokenization_distilbert because of the following error (look up to see its traceback):\ncannot import name '__version__' from 'tokenizers.tokenizers' (/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tokenizers/tokenizers.cpython-310-darwin.so)"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_tf = [tokenizer(text, padding='max_length', max_length = 512, truncation=True)['input_ids'] for text in X]\n",
    "X_tf = np.array(X_tf, dtype='int32')\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Shape of training data: ',X_tf_train.shape)\n",
    "print('Shape of test data: ',X_tf_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
