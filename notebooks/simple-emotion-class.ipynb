{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.9)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.9)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "notebooks_dir = Path.cwd()\n",
    "project_dir = notebooks_dir.parent\n",
    "data_dir = project_dir / 'data' / 'raw'\n",
    "text_data_path = data_dir / 'Subtask_2_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.9)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text_data = json.loads(text_data_path.read_text())\n",
    "for i in range(len(text_data[0]['conversation'])):\n",
    "    print(text_data[0]['conversation'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandler (neutral): Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .\n",
      "All (neutral): Oh , yeah . Had that dream .\n",
      "Chandler (surprise): Then I look down , and I realize there is a phone ... there .\n",
      "Joey (surprise): Instead of ... ?\n",
      "Chandler (anger): That is right .\n",
      "Joey (neutral): Never had that dream .\n",
      "Phoebe (neutral): No .\n",
      "Chandler (neutral): All of a sudden , the phone starts to ring .\n"
     ]
    }
   ],
   "source": [
    "def print_format_conv(json_convo):\n",
    "    for i in range(len(json_convo)):\n",
    "        print(f\"{json_convo[i]['speaker']} ({json_convo[i]['emotion']}): {json_convo[i]['text']}\")\n",
    "\n",
    "print_format_conv(text_data[0]['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract conversation lines and put them into a single string with special sol and eol tokens\n",
    "# and extract emotions to list \n",
    "def get_conversation_from_json(json_convo):\n",
    "    convo = []\n",
    "    emotions = []\n",
    "    for line_idx in range(len(json_convo)):\n",
    "        if line_idx == 0:\n",
    "            convo += ['<soc> <sol> ' + json_convo[line_idx]['text'] + ' <eol> ']\n",
    "        elif line_idx + 1 == len(json_convo):\n",
    "            convo += ['<sol> ' + json_convo[line_idx]['text'] + ' <eol> <eoc>']\n",
    "        else:\n",
    "            convo += ['<sol> ' + json_convo[line_idx]['text'] + ' <eol> ']\n",
    "        emotions += [json_convo[line_idx]['emotion']]\n",
    "    return convo, emotions\n",
    "\n",
    "convos = []\n",
    "all_emotions = []\n",
    "for convo_idx in range(len(text_data)):\n",
    "    convo, emotions = get_conversation_from_json(text_data[convo_idx]['conversation'])\n",
    "    convos += convo\n",
    "    all_emotions += emotions\n",
    "\n",
    "data = {'conversation' : convos, 'emotion': all_emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;soc&gt; &lt;sol&gt; Alright , so I am back in high sch...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sol&gt; Oh , yeah . Had that dream . &lt;eol&gt;</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sol&gt; Then I look down , and I realize there i...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sol&gt; Instead of ... ? &lt;eol&gt;</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sol&gt; That is right . &lt;eol&gt;</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation   emotion\n",
       "0  <soc> <sol> Alright , so I am back in high sch...   neutral\n",
       "1          <sol> Oh , yeah . Had that dream . <eol>    neutral\n",
       "2  <sol> Then I look down , and I realize there i...  surprise\n",
       "3                      <sol> Instead of ... ? <eol>   surprise\n",
       "4                       <sol> That is right . <eol>      anger"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/teodorastereciu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports for text cleaning\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# helper function to convert the pos tag format into something compatible with the lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to clean the text of a conversation\n",
    "def clean_text(doc, lemmatizer):\n",
    "    # expand contractions\n",
    "    doc = contractions.fix(doc)\n",
    "    \n",
    "    # remove every char that is not alphanumeric, keep spaces\n",
    "    doc = re.sub(r'\\W+', ' ', doc)\n",
    "    \n",
    "    # tokenize into all lowercase tokens\n",
    "    tokens = wordpunct_tokenize(doc) \n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    clean_tokens = ['<' + token + '>' if token in ['sol', 'eol', 'soc', 'eoc'] else token for token in tokens]\n",
    "    \n",
    "    # lemmatize tokens based on pos tags\n",
    "    # aka get base form \n",
    "    '''pos = pos_tag(tokens)\n",
    "    clean_tokens = []\n",
    "    for item in pos:\n",
    "        word = item[0]\n",
    "        tag = item[1]\n",
    "        if word in ['sol', 'eol']:\n",
    "            clean_tokens.append('<' + word + '>')\n",
    "        else:\n",
    "            clean_tokens.append(lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
    "    \n",
    "    clean_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
    "    '''\n",
    "\n",
    "    # add start of convo and end of convo markers\n",
    "    #clean_conv_toks = ['<soc>'] + clean_tokens + ['<eoc>']\n",
    "    return clean_tokens\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "df['clean_conversation'] = df['conversation'].apply(lambda convo: clean_text(convo, wn_lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        conversation   emotion  \\\n",
      "0  <soc> <sol> Alright , so I am back in high sch...   neutral   \n",
      "1          <sol> Oh , yeah . Had that dream . <eol>    neutral   \n",
      "2  <sol> Then I look down , and I realize there i...  surprise   \n",
      "3                      <sol> Instead of ... ? <eol>   surprise   \n",
      "4                       <sol> That is right . <eol>      anger   \n",
      "\n",
      "                                  clean_conversation  \n",
      "0  [<soc>, <sol>, alright, so, i, am, back, in, h...  \n",
      "1         [<sol>, oh, yeah, had, that, dream, <eol>]  \n",
      "2  [<sol>, then, i, look, down, and, i, realize, ...  \n",
      "3                        [<sol>, instead, of, <eol>]  \n",
      "4                    [<sol>, that, is, right, <eol>]  \n",
      "Vocabulary size (including special tokens): 6380\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "all_tokens = df['clean_conversation'].explode().tolist()\n",
    "unique_count = len(set(all_tokens))\n",
    "print(f\"Vocabulary size (including special tokens): {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# See TensorFlow version\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/__init__.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/__init__.py:37\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/__init__.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/__init__.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/compat/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookup\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/lite/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.lite namespace.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interpreter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpsSet\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/lite/experimental/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.lite.experimental namespace.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m authoring\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelAnalyzer \u001b[38;5;28;01mas\u001b[39;00m Analyzer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpResolverType\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.9)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "sequence_length = 30\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = df['clean_conversation'].tolist()\n",
    "train_label = df['emotion'].tolist()\n",
    "\n",
    "print(train_text)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "print(len(vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion recognition with a transformer-based encoder\n",
    "Input is text of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "notebooks_dir = Path.cwd()\n",
    "project_dir = notebooks_dir.parent\n",
    "data_dir = project_dir / 'data' / 'raw'\n",
    "text_data_path = data_dir / 'Subtask_2_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'utterance_ID': 1, 'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .', 'speaker': 'Chandler', 'emotion': 'neutral', 'video_name': 'dia1utt1.mp4'}\n",
      "{'utterance_ID': 2, 'text': 'Oh , yeah . Had that dream .', 'speaker': 'All', 'emotion': 'neutral', 'video_name': 'dia1utt2.mp4'}\n",
      "{'utterance_ID': 3, 'text': 'Then I look down , and I realize there is a phone ... there .', 'speaker': 'Chandler', 'emotion': 'surprise', 'video_name': 'dia1utt3.mp4'}\n",
      "{'utterance_ID': 4, 'text': 'Instead of ... ?', 'speaker': 'Joey', 'emotion': 'surprise', 'video_name': 'dia1utt4.mp4'}\n",
      "{'utterance_ID': 5, 'text': 'That is right .', 'speaker': 'Chandler', 'emotion': 'anger', 'video_name': 'dia1utt5.mp4'}\n",
      "{'utterance_ID': 6, 'text': 'Never had that dream .', 'speaker': 'Joey', 'emotion': 'neutral', 'video_name': 'dia1utt6.mp4'}\n",
      "{'utterance_ID': 7, 'text': 'No .', 'speaker': 'Phoebe', 'emotion': 'neutral', 'video_name': 'dia1utt7.mp4'}\n",
      "{'utterance_ID': 8, 'text': 'All of a sudden , the phone starts to ring .', 'speaker': 'Chandler', 'emotion': 'neutral', 'video_name': 'dia1utt8.mp4'}\n"
     ]
    }
   ],
   "source": [
    "text_data = json.loads(text_data_path.read_text())\n",
    "for i in range(len(text_data[0]['conversation'])):\n",
    "    print(text_data[0]['conversation'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandler (neutral): Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .\n",
      "All (neutral): Oh , yeah . Had that dream .\n",
      "Chandler (surprise): Then I look down , and I realize there is a phone ... there .\n",
      "Joey (surprise): Instead of ... ?\n",
      "Chandler (anger): That is right .\n",
      "Joey (neutral): Never had that dream .\n",
      "Phoebe (neutral): No .\n",
      "Chandler (neutral): All of a sudden , the phone starts to ring .\n"
     ]
    }
   ],
   "source": [
    "def print_format_conv(json_convo):\n",
    "    for i in range(len(json_convo)):\n",
    "        print(f\"{json_convo[i]['speaker']} ({json_convo[i]['emotion']}): {json_convo[i]['text']}\")\n",
    "\n",
    "print_format_conv(text_data[0]['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract conversation lines and put them into a single string with special sol and eol tokens\n",
    "# and extract emotions to list \n",
    "def get_conversation_from_json(json_convo):\n",
    "    convo = []\n",
    "    emotions = []\n",
    "    for line_idx in range(len(json_convo)):\n",
    "        if line_idx == 0:\n",
    "            convo += ['<soc> <sol> ' + json_convo[line_idx]['text'] + ' <eol> ']\n",
    "        elif line_idx + 1 == len(json_convo):\n",
    "            convo += ['<sol> ' + json_convo[line_idx]['text'] + ' <eol> <eoc>']\n",
    "        else:\n",
    "            convo += ['<sol> ' + json_convo[line_idx]['text'] + ' <eol> ']\n",
    "        emotions += [json_convo[line_idx]['emotion']]\n",
    "    return convo, emotions\n",
    "\n",
    "convos = []\n",
    "all_emotions = []\n",
    "for convo_idx in range(len(text_data)):\n",
    "    convo, emotions = get_conversation_from_json(text_data[convo_idx]['conversation'])\n",
    "    convos += convo\n",
    "    all_emotions += emotions\n",
    "\n",
    "data = {'conversation' : convos, 'emotion': all_emotions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13619\n",
      "13619\n"
     ]
    }
   ],
   "source": [
    "print(len(data['conversation']))\n",
    "print(len(data['emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = pd.DataFrame.from_dict(data)\n",
    "df.head()\n",
    "\n",
    "# imports for text cleaning\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# helper function to convert the pos tag format into something compatible with the lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# used to clean the text of a conversation\n",
    "def clean_text(doc, lemmatizer):\n",
    "    # expand contractions\n",
    "    doc = contractions.fix(doc)\n",
    "    \n",
    "    # remove every char that is not alphanumeric, keep spaces\n",
    "    doc = re.sub(r'\\W+', ' ', doc)\n",
    "    \n",
    "    # tokenize into all lowercase tokens\n",
    "    tokens = wordpunct_tokenize(doc) \n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    clean_tokens = ['<' + token + '>' if token in ['sol', 'eol'] else token for token in tokens]\n",
    "    \n",
    "    # lemmatize tokens based on pos tags\n",
    "    # aka get base form \n",
    "    ''''''pos = pos_tag(tokens)\n",
    "    clean_tokens = []\n",
    "    for item in pos:\n",
    "        word = item[0]\n",
    "        tag = item[1]\n",
    "        if word in ['sol', 'eol']:\n",
    "            clean_tokens.append('<' + word + '>')\n",
    "        else:\n",
    "            clean_tokens.append(lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
    "    #clean_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
    "    '''\n",
    "\n",
    "    # add start of convo and end of convo markers\n",
    "    '''clean_conv_toks = ['<soc>'] + clean_tokens + ['<eoc>']\n",
    "    return clean_conv_toks\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "df['clean_conversation'] = df['conversation'].apply(lambda convo: clean_text(convo, wn_lemmatizer))'''\n",
    "\n",
    "#df.head()\n",
    "\n",
    "# all_tokens = df['clean_conversation'].explode().tolist()\n",
    "# unique_count = len(set(all_tokens))\n",
    "# print(f\"Vocabulary size (including special tokens): {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "vocab_size = 6380\n",
    "embedding_dim = 100\n",
    "max_seq_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation line: <soc> <sol> Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked . <eol> \n",
      "Emotions: b'neutral'\n",
      "Conversation line: <sol> Oh , yeah . Had that dream . <eol> \n",
      "Emotions: b'neutral'\n",
      "Conversation line: <sol> Then I look down , and I realize there is a phone ... there . <eol> \n",
      "Emotions: b'surprise'\n"
     ]
    }
   ],
   "source": [
    "# convert to df dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[\"conversation\"], data[\"emotion\"]))\n",
    "\n",
    "# Print a sample from the dataset\n",
    "for conversation, emotions in dataset.take(3):\n",
    "    print(\"Conversation line:\", conversation.numpy().decode(\"utf-8\"))\n",
    "    print(\"Emotions:\", emotions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<soc> <sol> Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked . <eol> ', '<sol> Oh , yeah . Had that dream . <eol> ', '<sol> Then I look down , and I realize there is a phone ... there . <eol> ', '<sol> Instead of ... ? <eol> ', '<sol> That is right . <eol> ', '<sol> Never had that dream . <eol> ', '<sol> No . <eol> ', '<sol> All of a sudden , the phone starts to ring . <eol> <eoc>', '<soc> <sol> I do not want to be single , okay ? I just ... I just ... I just wanna be married again ! <eol> ', '<sol> And I just want a million dollars ! <eol> ']\n",
      "['neutral', 'neutral', 'surprise', 'surprise', 'anger', 'neutral', 'neutral', 'neutral', 'sadness', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "train_examples = []\n",
    "train_labels = []\n",
    "\n",
    "for conversation_line, emotion in dataset:\n",
    "    train_examples.append(conversation_line.numpy().decode(\"utf-8\"))\n",
    "    train_labels.append(emotion.numpy().decode(\"utf-8\"))\n",
    "\n",
    "print(train_examples[:10])\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def fix_contractions(input_data):\n",
    "    return contractions.fix(input_data.numpy().decode(\"utf-8\"))\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  fixed_contractions = tf.py_function(fix_contractions, [input_data], tf.string)\n",
    "  lowercase = tf.strings.lower(fixed_contractions)\n",
    "  # remove non-alphanumeric characters except '<' and '>'\n",
    "  cleaned_text = tf.strings.regex_replace(lowercase, '[^a-zA-Z0-9<>]', ' ')\n",
    "  # remove extra spaces\n",
    "  cleaned_text = tf.strings.regex_replace(cleaned_text, ' +', ' ')\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Conversation: <soc> <sol> Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked . <eol> \n",
      "Original Emotions: b'neutral'\n",
      "Standardized Conversation: <soc> <sol> alright so i am back in high school i am standing in the middle of the cafeteria and i realize i am totally naked <eol> \n"
     ]
    }
   ],
   "source": [
    "for conversation, emotions in dataset.take(1):\n",
    "    print(\"Original Conversation:\", conversation.numpy().decode(\"utf-8\"))\n",
    "    print(\"Original Emotions:\", emotions.numpy())\n",
    "\n",
    "    # Apply custom standardization\n",
    "    standardized_conversation = custom_standardization(conversation)\n",
    "    print(\"Standardized Conversation:\", standardized_conversation.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "sequence_length = 30\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    #max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'sol', 'eol', 'i', 'you', 'is', 'the', 'not', 'it', 'to', 'a', 'that', 'and', 'do', 'oh', 'are', 'soc', 'what', 'eoc', 'am', 'no', 'know', 'have', 'we', 'me', 'this', 'so', 'my', 'okay', 'just', 'of', 'yeah', 'in', 'can', 'well', 'on', 'hey', 'was', 'all', 'for', 'with', 'right', 'he', 'be', 'but', 'will', 'your', 'here', 'like', 'go', 'there', 'did', 'get', 'out', 'gonna', 'would', 'about', 'uh', 'she', 'really', 'think', 'up', 'one', 'look', 'if', 'how', 'now', 'they', 'got', 'her', 'at', 'mean', 'see', 'him', 'ross', 'come', 'why', 'good', 'god', 'want', 'sorry', 'joey', 'going', 'could', 'who', 'tell', 'great', 'when', 'let', 'guys', 'hi', 'then', 'chandler', 'yes', 'time', 'were', 'umm', 'because', 'does', 'say', 'had', 'some', 'little', 'should', 'back', 'love', 'too', 'over', 'something', 'rachel', 'monica', 'guy', 'wait', 'been', 'doing', 'make', 'from', 'an', 'or', 'as', 'said', 'phoebe', 'where', 'them', 'ok', 'maybe', 'sure', 'wanna', 'never', 'ah', 'huh', 'thing', 'thank', 'people', 'us', 'take', 'his', 'again', 'way', 'man', 'much', 'need', 'two', 'still', 'please', 'listen', 'by', 'very', 'believe', 'thought', 'down', 'off', 'fine', 'wow', 'our', 'give', 'more', 'big', 'even', 'baby', 'um', 'has', 'actually', 'stop', 'first', 'work', 'thanks', 'gotta', 'whoa', 'feel', 'call', 'any', 'night', 'before', 'only', 'nice', 'talk', 'anything', 'other', 'lot', 'pheebs', 'ohh', 'new', 'these', 'stuff', 'ever', 'last', 'room', 'bye', 'those', 'put', 'nothing', 'honey', 'place', 'together', 'girl', 'better', 'hello', 'pretty', 'things', 'remember', 'guess', 'talking', 'wanted', 'tonight', 'name', 'care', 'married', 'kinda', 'help', 'around', 'went', 'stupid', 'which', 'told', 'play', 'left', 'into', 'having', 'woman', 'day', 'best', 'bad', 'rach', 'thinking', 'cause', 'always', 'happy', 'happened', 'friend', 'ooh', 'getting', 'try', 'made', 'fun', 'totally', 'kind', 'ask', 'wedding', 'someone', 'phone', 'mom', 'friends', 'next', 'might', 'find', 'after', 'done', 'date', 'coming', 'than', 'second', 'keep', 'everything', 'crazy', 'whole', 'today', 'pick', 'looking', 'funny', 'sex', 'long', 'wrong', 'stay', 'ow', 'job', 'hear', 'game', 'five', 'else', 'check', 'years', 'ring', 'ready', 'problem', 'old', 'coffee', 'anyway', 'later', 'home', 'hard', 'enough', 'course', 'came', 'yet', 'trying', 'called', 'another', 'probably', 'myself', 'life', 'away', 'meet', 'everybody', 'every', 'women', 'three', 'such', 'happen', 'wants', 'tomorrow', 'minute', 'idea', 'head', 'part', 'move', 'hand', 'apartment', 'through', 'saying', 'real', 'hot', 'excuse', 'eat', 'ben', 'being', 'took', 'mr', 'live', 'leave', 'dad', 'cool', 'naked', 'morning', 'father', 'alright', 'since', 'money', 'mon', 'mine', 'year', 'supposed', 'looks', 'both', 'bing', 'anyone', 'weird', 'watch', 'show', 'serious', 'same', 'may', 'making', 'kidding', 'high', 'either', 'each', 'use', 'somebody', 'sleep', 'person', 'party', 'number', 'must', 'most', 'hours', 'hold', 'ho', 'deal', 'beautiful', 'their', 'saw', 'doctor', 'world', 'telling', 'sounds', 'seen', 'joe', 'house', 'dinner', 'break', 'though', 'start', 'lost', 'late', 'hell', 'found', 'dr', 'bring', 'bed', 'anymore', 'worry', 'understand', 'turn', 'story', 'question', 'matter', 'knew', 'ha', 'gave', 'couple', 'also', 'already', 'while', 'until', 'says', 'marry', 'janice', 'half', 'geller', 'four', 'easy', 'dude', 'babies', 'amazing', 'true', 'throw', 'ten', 'sweet', 'school', 'makes', 'kiss', 'hate', 'gone', 'carol', 'yourself', 'wish', 'used', 'open', 'movie', 'knows', 'kill', 'hmm', 'hang', 'forget', 'comes', 'alone', 'absolutely', 'walk', 'sister', 'picture', 'own', 'o', 'minutes', 'mind', 'heard', 'chance', 'bit', 'without', 'read', 'mad', 'kids', 'goes', 'glad', 'free', 'face', 'cute', 'c', 'bet', 'ahh', 'wife', 'wearing', 'wear', 'perfect', 'once', 'hurt', 'gay', 'exactly', 'e', 'different', 'cut', 'bob', 'sweetie', 'pants', 'miss', 'hour', 'girlfriend', 'everyone', 'dance', 'cannot', 'asked', 'anybody', 'week', 'uhh', 'ugh', 'sit', 'car', 'brother', 'between', 'wh', 'times', 'started', 'promise', 'parents', 'office', 'fact', 'em', 'close', 'book', 'ball', 'aw', 'able', 'working', 'under', 'six', 'pregnant', 'moving', 'lady', 'hope', 'girls', 'fire', 'end', 'eh', 'boy', 'ass', 'touch', 'song', 'seven', 'relax', 'men', 'kid', 'important', 'hair', 'gets', 'fight', 'dollars', 'clothes', 'broke', 'bathroom', 'asleep', 'whatever', 'welcome', 'talked', 'taking', 'table', 'soon', 'seeing', 'reason', 'playing', 'moment', 'marriage', 'lives', 'hit', 'grandmother', 'food', 'buy', 'buddy', 'ago', 'actor', 'yep', 'win', 'susan', 'store', 'side', 'set', 'run', 'restaurant', 'nobody', 'mother', 'means', 'many', 'machine', 'liked', 'joke', 'huge', 'hands', 'figured', 'fault', 'family', 'eyes', 'dog', 'days', 'david', 'class', 'aww', 'yours', 'write', 'unbelievable', 'top', 'stick', 'son', 'sir', 'sick', 'seem', 'roommate', 'red', 'plan', 'pay', 'museum', 'green', 'city', 'building', 'bitch', 'asking', 'answer', '30', 'weekend', 'tribbiani', 'thanksgiving', 'terrible', 'street', 'spent', 'small', 'shower', 'shoot', 'seems', 's', 'paolo', 'ohhh', 'message', 'loved', 'lose', 'living', 'joshua', 'idiot', 'forgot', 'floor', 'few', 'feeling', 'fast', 'eight', 'door', 'change', 'candy', 'brought', 'blue', 'bag', 'audition', 'word', 'weeks', 'stuck', 'smell', 'seriously', 'sad', 'quick', 'questions', 'plans', 'milk', 'luck', 'line', 'kathy', 'invite', 'hospital', 'freak', 'felt', 'die', 'definitely', 'dead', 'chick', 'cat', 'card', 'bucks', 'underwear', 'tickets', 'thinks', 'team', 'stole', 'spend', 'special', 'shut', 'scared', 'save', 'richard', 'realize', 'quit', 'point', 'pete', 'moved', 'meant', 'mark', 'lunch', 'light', 'leaving', 'least', 'its', 'husband', 'grade', 'grab', 'finish', 'figure', 'except', 'emily', 'drink', 'damn', 'cup', 'completely', 'chef', 'calling', 'boyfriend', 'bigger', 'apparently', '8', 'yesterday', 'wrote', 'waiting', 'tired', 'taste', 'surprise', 'straight', 'stand', 'soup', 'shot', 'shirt', 'scene', 'rules', 'rest', 'relationship', 'porn', 'pass', 'nope', 'nickname', 'mrs', 'months', 'list', 'julie', 'ice', 'heart', 'heads', 'guest', 'gimme', 'front', 'fell', 'feels', 'eye', 'excited', 'ew', 'decide', 'cry', 'clock', 'clean', 'box', 'birthday', 'along', 'almost', 'against', 'yay', 'woo', 'wondering', 'upset', 'turned', 'trust', 'third', 'tag', 'sound', 'somewhere', 'sometimes', 'sitting', 'romantic', 'regular', 'ran', 'plus', 'paper', 'ones', 'news', 'looked', 'laundry', 'interesting', 'happens', 'full', 'finally', 'feelings', 'favorite', 'fake', 'fair', 'entire', 'dum', 'drunk', 'divorce', 'desk', 'damnit', 'daddy', 'cookies', 'chip', 'character', 'boxes', 'bought', 'behind', 'although', '2', 'worried', 'won', 'window', 'water', 'watching', 'using', 'turns', 'town', 'switch', 'sweater', 'sucks', 'suck', 'strong', 'speak', 'sometime', 'shoes', 'secret', 'pizza', 'park', 'obviously', 'nooo', 'noo', 'nine', 'missed', 'mess', 'listening', 'leg', 'knock', 'king', 'keys', 'kay', 'jill', 'interested', 'hoo', 'gunther', 'guitar', 'fall', 'duck', 'drop', 'drake', 'decided', 'crush', 'children', 'calm', 'birds', 'apologize', '10', 'yemen', 'wonderful', 'tv', 'tiny', 'tape', 'stripper', 'stopped', 'starts', 'smart', 'rule', 'route', 'roger', 'purse', 'pull', 'professor', 'pictures', 'past', 'page', 'owe', 'needs', 'mona', 'mistake', 'middle', 'mail', 'ma', 'london', 'likes', 'lie', 'ladies', 'kick', 'i’m', 'hundred', 'hole', 'gotten', 'g', 'freaked', 'fish', 'far', 'fan', 'familiar', 'enjoy', 'early', 'difference', 'dear', 'dating', 'calls', 'button', 'business', 'bunch', 'afraid', 'across', 'york', 'y', 'worse', 'wonder', 'vegas', 'unless', 'uncle', 'turkey', 'trip', 'tough', 'test', 'teacher', 'takes', 'surprised', 'star', 'spending', 'speed', 'speech', 'sleeping', 'sing', 'shh', 'sexual', 'sent', 'sense', 'seconds', 'roll', 'robot', 'ride', 'presents', 'porsche', 'played', 'places', 'piece', 'pack', 'outside', 'nervous', 'neither', 'names', 'month', 'met', 'maid', 'lucky', 'loves', 'lights', 'learned', 'learn', 'kissed', 'kept', 'jam', 'instead', 'inside', 'hoping', 'health', 'giving', 'frank', 'forever', 'foot', 'feet', 'favor', 'fat', 'ex', 'empty', 'emma', 'dress', 'downstairs', 'dirty', 'died', 'dancing', 'couch', 'contracts', 'comfortable', 'coat', 'cheese', 'changed', 'catch', 'case', 'broken', 'body', 'birth', 'barry', 'bank', 'band', 'awkward', 'ate', 'assistant', 'alan', '7', '3', '00', 'worth', 'worst', 'worked', 'words', 'wine', 'white', 'wendy', 'ways', 'walking', 'twice', 'twenty', 'tulsa', 'truth', 'trouble', 'tried', 'throwing', 'thoughts', 'teach', 'system', 'step', 'starting', 'space', 'short', 'shopping', 'share', 'sell', 'saturday', 'safe', 'remembered', 'reading', 'quite', 'quarter', 'proud', 'post', 'possible', 'plane', 'paid', 'nuts', 'notice', 'nah', 'music', 'movies', 'mouth', 'monday', 'meeting', 'magic', 'lying', 'lovely', 'lots', 'less', 'larry', 'la', 'key', 'joseph', 'jokes', 'jealous', 'james', 'j', 'issac', 'imagine', 'human', 'hotel', 'holiday', 'happening', 'hanging', 'gift', 'fashion', 'experience', 'especially', 'embarrassed', 'elizabeth', 'drive', 'don’t', 'deep', 'daughter', 'cream', 'crap', 'cooking', 'college', 'clear', 'christmas', 'choose', 'chicken', 'checking', 'charming', 'celebrate', 'casey', 'cares', 'careful', 'bunny', 'bugs', 'buffay', 'breast', 'breaking', 'brain', 'borrow', 'books', 'bonnie', 'bike', 'bedroom', 'bar', 'awww', 'anywhere', 'annulment', 'angela', 'allowed', 'age', 'acting', '50', '1', 'you’re', 'young', 'writing', 'works', 'warm', 'walked', 'wake', 'waitress', 'view', 'uterus', 'universe', 'ugly', 'tree', 'trash', 'toby', 'tests', 'technically', 'tall', 'sudden', 'subway', 'strip', 'standing', 'stalin', 'spot', 'sort', 'soo', 'smoke', 'smile', 'showing', 'sensitive', 'send', 'sandwich', 'rooms', 'rome', 'rather', 'putting', 'possibly', 'poor', 'plate', 'pie', 'personal', 'percent', 'pain', 'outta', 'otherwise', 'order', 'opened', 'needed', 'neck', 'n', 'ms', 'monkey', 'mm', 'million', 'mike', 'memories', 'mary', 'mac', 'letting', 'leslie', 'lesbian', 'keeps', 'juice', 'judge', 'italian', 'island', 'information', 'incredible', 'horrible', 'hooked', 'honor', 'honest', 'holding', 'hearing', 'handle', 'hall', 'h', 'genius', 'future', 'forward', 'follow', 'flowers', 'fix', 'fired', 'final', 'fighting', 'fear', 'explain', 'expect', 'excellent', 'europe', 'ended', 'easier', 'earrings', 'dump', 'dumb', 'dry', 'dream', 'decision', 'd', 'crying', 'cowboy', 'cover', 'conversation', 'congratulations', 'company', 'club', 'choice', 'child', 'cheesecake', 'central', 'camera', 'cab', 'busy', 'bug', 'bride', 'boat', 'bijan', 'basically', 'bald', 'aunt', 'attention', 'appreciate', 'actual', 'accept', '6', '4', 'whose', 'whenever', 'wha', 'wall', 'vic', 'uncomfortable', 'tie', 'ticket', 'tea', 'study', 'staying', 'sorta', 'sold', 'size', 'situation', 'single', 'silly', 'sheet', 'sexy', 'secrets', 'sec', 'seat', 'script', 'sandwiches', 'robert', 'rip', 'return', 'remoray', 'realized', 'realise', 'r', 'princess', 'pressure', 'pocket', 'plastic', 'pig', 'picked', 'perhaps', 'paying', 'paul', 'op', 'oooh', 'oo', 'numbers', 'noodle', 'noise', 'nap', 'molly', 'missing', 'lover', 'longer', 'license', 'letter', 'lean', 'lately', 'land', 'killing', 'kicking', 'kicked', 'john', 'jason', 'jail', 'invited', 'inappropriate', 'hurry', 'hung', 'horny', 'honeymoon', 'gym', 'gun', 'groom', 'greatest', 'grandma', 'given', 'giant', 'garbage', 'games', 'flight', 'fit', 'evil', 'everyday', 'enjoying', 'engaged', 'during', 'dumped', 'dressed', 'double', 'divorced', 'disgusting', 'dentist', 'cups', 'cross', 'credit', 'copy', 'cook', 'control', 'coats', 'closed', 'clint', 'chi', 'center', 'bus', 'british', 'boutros', 'bottom', 'boss', 'bond', 'board', 'bite', 'biggest', 'become', 'beach', 'bastard', 'backup', 'available', 'asks', 'area', 'annoying', 'air', 'ahead', 'afternoon', 'admit', 'act', 'accident', '9', '200', '19', 'yelling', 'yell', 'whoever', 'waiter', 'vestibule', 'usually', 'ursula', 'uniform', 'unfair', 'typical', 'twins', 'twelve', 'tux', 'trick', 'treeger', 'treat', 'train', 'toy', 'towel', 'til', 'tight', 'thursday', 'threw', 'teaching', 'tasted', 'taken', 'switching', 'swing', 'swear', 'suppose', 'suds', 'sucked', 'stars', 'staring', 'stage', 'sports', 'split', 'sperm', 'soap', 'snack', 'smelly', 'slut', 'slow', 'slept', 'sink', 'sign', 'shop', 'shoe', 'shame', 'selling', 'seemed', 'security', 'scary', 'saving', 'saved', 'sauce', 'santa', 'san', 'salmon', 'salad', 'rock', 'road', 'ridiculous', 'rid', 'reset', 'ralph', 'raise', 'quitting', 'push', 'puck', 'private', 'pretending', 'practice', 'power', 'poker', 'poem', 'pink', 'pieces', 'picking', 'pee', 'pal', 'pages', 'p', 'omnipotent', 'nothin', 'note', 'none', 'nipple', 'negative', 'natural', 'named', 'mouse', 'minsk', 'massage', 'major', 'loud', 'loose', 'lived', 'lift', 'lemonade', 'leather', 'kyle', 'knicks', 'kitchen', 'killed', 'keeping', 'jordie', 'jerk', 'jenny', 'jake', 'jack', 'it’s', 'isabella', 'interview', 'international', 'insurance', 'hurts', 'hormones', 'hooker', 'hm', 'hitting', 'hers', 'hero', 'helping', 'heldi', 'hats', 'hates', 'hat', 'handling', 'hah', 'goodbye', 'gee', 'gary', 'french', 'football', 'fly', 'fill', 'favourite', 'falling', 'ezel', 'extra', 'expecting', 'exciting', 'embarrassing', 'eating', 'dying', 'drinks', 'drinking', 'disneyland', 'disagree', 'dinosaur', 'dial', 'device', 'deserve', 'department', 'de', 'dark', 'danny', 'creep', 'covered', 'court', 'computer', 'commitment', 'cold', 'closer', 'clearly', 'chute', 'chloe', 'chicago', 'charity', 'chair', 'cha', 'centimeters', 'caught', 'cards', 'canoe', 'candles', 'cancel', 'built', 'bucket', 'brown', 'breathe', 'brave', 'boring', 'boobs', 'bobby', 'blew', 'black', 'bell', 'beer', 'bee', 'beat', 'bear', 'bake', 'bags', 'attractive', 'attic', 'atm', 'arm', 'apple', 'apartments', 'america', 'airport', 'ad', 'actors', 'account', '5', '40', '27', '25', '17', '15', 'zoo', 'zelner', 'wind', 'wet', 'wayne', 'wash', 'war', 'wanting', 'virgin', 'views', 'video', 'vase', 'upstairs', 'unit', 'unagi', 'u', 'type', 'twin', 'trade', 'tour', 'touched', 'toilet', 'tips', 'thread', 'thirty', 'terrific', 'television', 'teaches', 'tank', 'talks', 'talkin', 'sweetheart', 'suit', 'suggestion', 'suddenly', 'student', 'stomach', 'sticky', 'sticks', 'stevens', 'steal', 'states', 'state', 'stare', 'stain', 'squeeze', 'square', 'specials', 'sooo', 'sneak', 'smooth', 'slide', 'slice', 'skin', 'sisters', 'showed', 'shave', 'sharp', 'service', 'semi', 'sees', 'section', 'sea', 'screwed', 'screw', 'scientist', 'science', 'schedule', 'scare', 'sale', 'ruin', 'routine', 'rosselini', 'roommates', 'roof', 'rogers', 'ripped', 'rich', 'rhyme', 'review', 'respect', 'rain', 'quickly', 'quality', 'quack', 'pure', 'punch', 'public', 'psyched', 'project', 'price', 'pretend', 'present', 'prefer', 'poughkeepsie', 'pot', 'popular', 'pool', 'points', 'playstation', 'planning', 'physical', 'peanut', 'partner', 'paris', 'pair', 'ours', 'ordered', 'opportunity', 'often', 'obvious', 'nurse', 'noticed', 'nose', 'noooo', 'noises', 'nipples', 'neighbors', 'needle', 'near', 'montreal', 'mix', 'misunderstood', 'millions', 'market', 'marcel', 'magazines', 'm', 'lovers', 'losing', 'loser', 'litter', 'limit', 'lily', 'licked', 'library', 'liam', 'letters', 'legs', 'league', 'lawyer', 'laughing', 'laugh', 'ladle', 'labor', 'known', 'kitty', 'kip', 'junior', 'jump', 'join', 'jim', 'jane', 'itself', 'insane', 'indian', 'incident', 'hypothetically', 'hungry', 'hug', 'host', 'hooking', 'history', 'hired', 'hide', 'heh', 'heaven', 'healthy', 'heading', 'guard', 'grown', 'growing', 'grow', 'group', 'ground', 'greg', 'gosh', 'gorgeous', 'goofing', 'goodacre', 'goggles', 'glass', 'george', 'gene', 'gang', 'furniture', 'fruit', 'friendly', 'fridge', 'friday', 'freaking', 'fourth', 'forty', 'fortunately', 'forgive', 'forbidden', 'following', 'fling', 'finds', 'finals', 'filled', 'fifth', 'fifteen', 'female', 'feeding', 'farber', 'fancy', 'famous', 'expensive', 'everywhere', 'evening', 'estelle', 'era', 'entertainment', 'enjoyed', 'engagement', 'energy', 'dropper', 'dropped', 'doctors', 'director', 'difficult', 'diet', 'diego', 'didn’t', 'delivery', 'death', 'dates', 'dare', 'danielle', 'dana', 'dammit', 'crime', 'creeps', 'country', 'count', 'cost', 'considered', 'confident', 'conference', 'commercial', 'comin', 'comet', 'combination', 'collection', 'coke', 'cobb', 'co', 'closing', 'click', 'classes', 'circles', 'chuck', 'chances', 'chan', 'certainly', 'carrying', 'cameras', 'caller', 'burn', 'brolin', 'breasts', 'breakfast', 'bra', 'boyfriends', 'bout', 'borrowed', 'born', 'blood', 'blocks', 'blind', 'blazer', 'bla', 'besides', 'beef', 'became', 'basket', 'based', 'base', 'bars', 'barn', 'barely', 'awful', 'attracted', 'aruba', 'arms', 'appointment', 'apart', 'answering', 'anniversary', 'angry', 'altar', 'alive', 'ai', 'agreed', 'agent', 'advice', 'address', 'ace', 'accent', '20', 'younger', 'yore', 'ya', 'wrapped', 'worker', 'wise', 'winning', 'wildness', 'wild', 'whoo', 'whispering', 'whipped', 'whew', 'whack', 'western', 'wells', 'weirdest', 'weddings', 'waving', 'wasting', 'waste', 'waitressing', 'waiters', 'voice', 'visit', 'violated', 'vince', 'vent', 'valentine', 'unfortunately', 'tuesday', 'trusted', 'trifle', 'training', 'tons', 'ton', 'tomato', 'toes', 'toast', 'tip', 'tile', 'threatened', 'thquirt', 'thousand', 'thirsty', 'th', 'terry', 'tells', 'telethon', 'teeth', 'taught', 'tastes', 'tarragon', 'tan', 't', 'swim', 'sweeping', 'supportive', 'support', 'supplies', 'sunday', 'summer', 'sugar', 'success', 'students', 'strength', 'stranger', 'stories', 'sting', 'stephanopoulos', 'stealing', 'steady', 'station', 'stack', 'stable', 'spoon', 'spit', 'spilled', 'spill', 'specifics', 'spacecamp', 'sophie', 'songs', 'someplace', 'someday', 'soda', 'sock', 'smoking', 'smells', 'slip', 'sixth', 'sissone', 'silliness', 'sidney', 'sickness', 'siadic', 'shy', 'sheets', 'sexually', 'serve', 'sergei', 'separate', 'sells', 'seek', 'search', 'scrud', 'scottish', 'saves', 'sauti', 'sarah', 'sake', 'safely', 'russia', 'ruined', 'rude', 'row', 'rhonda', 'retract', 'results', 'responsibility', 'responsibilities', 'reservations', 'research', 'report', 'rent', 'remove', 'remind', 'related', 'reject', 'recipe', 'reasons', 'reached', 'raymond', 'radio', 'queen', 'quarters', 'pushing', 'purpose', 'puppet', 'pulling', 'promised', 'professional', 'product', 'problems', 'privacy', 'press', 'president', 'pregnancy', 'pound', 'postpone', 'pong', 'poking', 'planned', 'planet', 'pit', 'ping', 'photographer', 'phoebs', 'phil', 'phase', 'personality', 'perform', 'peel', 'peeking', 'peeing', 'pearls', 'peach', 'patient', 'pathetic', 'pat', 'pardon', 'pancakes', 'paleontology', 'oy', 'oww', 'ourselves', 'options', 'opposite', 'opera', 'opening', 'ooooh', 'onion', 'oil', 'official', 'odds', 'ocean', 'oboe', 'oberman', 'oatmeal', 'nude', 'novels', 'notch', 'normal', 'nooooo', 'non', 'nights', 'nerve', 'needy', 'neat', 'navy', 'narrowed', 'nana', 'naming', 'musician', 'muscles', 'muffins', 'mountain', 'moon', 'mood', 'mmm', 'minister', 'mini', 'mindy', 'midnight', 'messed', 'mention', 'medical', 'mcdowell', 'matthews', 'match', 'master', 'marks', 'marion', 'manager', 'male', 'mailman', 'magioni', 'losers', 'locked', 'lock', 'local', 'lipstick', 'lips', 'lip', 'lines', 'lined', 'lilies', 'liking', 'lighter', 'lifetime', 'lied', 'lets', 'lesson', 'legitimate', 'lead', 'layer', 'law', 'lauren', 'laughed', 'large', 'lamp', 'laminated', 'kristin', 'kristen', 'knowing', 'knocking', 'knocked', 'kissing', 'kinds', 'ken', 'kash', 'kangaroo', 'jumping', 'july', 'julio', 'jr', 'jell', 'janine', 'jamie', 'jacket', 'issue', 'ironic', 'invented', 'intense', 'instincts', 'insist', 'incredibly', 'including', 'impression', 'impossible', 'identical', 'ideas', 'howie', 'howard', 'hopefully', 'homemade', 'hollow', 'holidays', 'hogging', 'himself', 'hiking', 'hidden', 'hernia', 'heckles', 'heathcliff', 'heat', 'havin', 'hated', 'harm', 'hardly', 'harder', 'handsome', 'gum', 'grocery', 'grass', 'gives', 'ginger', 'gin', 'ghost', 'gentlemen', 'geez', 'gate', 'gas', 'gali', 'gala', 'fund', 'freeze', 'frannie', 'francis', 'france', 'forth', 'forest', 'fool', 'focus', 'flying', 'flip', 'flesh', 'flea', 'fireman', 'finished', 'finger', 'film', 'filing', 'field', 'fellow', 'fantasy', 'fantastic', 'exit', 'exist', 'ewww', 'eww', 'evolution', 'evaluation', 'erin', 'entice', 'england', 'empire', 'emergency', 'eligible', 'eleven', 'eighth', 'ehh', 'egg', 'ed', 'easily', 'duties', 'ducks', 'drums', 'drugs', 'drew', 'drapes', 'dragon', 'docks', 'division', 'disease', 'dictionary', 'diaper', 'deeper', 'decaf', 'dealership', 'data', 'damage', 'customers', 'customer', 'crotch', 'critic', 'cried', 'creepy', 'cramp', 'cracking', 'crack', 'counter', 'countdown', 'coulda', 'correct', 'cookie', 'convinced', 'controls', 'contraction', 'conscious', 'concept', 'concentrate', 'complicated', 'complement', 'coma', 'color', 'coffeehouse', 'coast', 'clowns', 'clown', 'closet', 'clinic', 'client', 'clever', 'classy', 'cigarette', 'chop', 'chess', 'chelsea', 'checked', 'chasing', 'chase', 'charge', 'channel', 'changing', 'cervix', 'certain', 'century', 'celtics', 'catching', 'catalog', 'cast', 'cassie', 'carry', 'carl', 'career', 'canvas', 'cancellation', 'cake', 'cafeteria', 'butts', 'butt', 'burning', 'burned', 'brutal', 'brush', 'brunch', 'brothers', 'bringing', 'boxer', 'bowl', 'bother', 'boots', 'boom', 'boarding', 'blow', 'blouse', 'bloody', 'blond', 'blazers', 'blame', 'blackout', 'bird', 'biology', 'bill', 'betcha', 'bernie', 'begley', 'begin', 'beeped', 'beard', 'batch', 'barney’s', 'barcelona', 'bapstein', 'bamboozled', 'backhand', 'ba', 'b', 'awesome', 'aware', 'avoid', 'audience', 'assume', 'armadillo', 'arguing', 'answers', 'ancient', 'amy', 'ameri', 'alice', 'alarm', 'ahhhh', 'agree', 'afford', 'adams', 'adam', 'action', 'abbey', '80', '700', '300', '2000', '13', '100', '‘sup', 'zone', 'zana', 'y’know', 'yuh', 'you’ve', 'yourselves', 'yoghurt', 'yo', 'yikes', 'yelled', 'yee', 'ye', 'yasmine', 'wuss', 'written', 'writers', 'wrecking', 'wreck', 'wrap', 'wound', 'worrying', 'worn', 'woowoo', 'witness', 'wished', 'wire', 'windows', 'wig', 'whoops', 'whom', 'whitney', 'whistle', 'whip', 'wheel', 'westminster', 'wesley', 'welch', 'weight', 'weak', 'waxine', 'wax', 'wave', 'walls', 'waking', 'waited', 'waaay', 'wa', 'von', 'volvo', 'visiting', 'vision', 'virginity', 'village', 'victor', 'vibe', 'venture', 'vendor', 'van', 'vacuum', 'urse', 'urgent', 'upon', 'unusual', 'unstable', 'unreasonable', 'unhook', 'unfortunate', 'understands', 'understanding', 'underpants', 'underdog', 'unbelievably', 'unattended', 'unacceptable', 'uma', 'ultimate', 'ukrainian', 'twelfth', 'tweezers', 'turning', 'truck', 'triple', 'trees', 'treasures', 'traps', 'trapped', 'tramp', 'tragic', 'track', 'touching', 'total', 'topic', 'tooty', 'tongue', 'toner', 'tomatoes', 'tom', 'tin', 'tim', 'tiles', 'tiffany', 'thurman', 'thumb', 'thrilled', 'threesome', 'thinkin', 'thin', 'there’s', 'theory', 'themselves', 'theme', 'theatre', 'theater', 'that’s', 'testing', 'testify', 'terrace', 'terms', 'tequila', 'tellin', 'telescope', 'tar', 'taped', 'taller', 'tail', 'tad', 'tacos', 'tables', 'symbolism', 'swoop', 'switched', 'swimming', 'sweety', 'sweetest', 'sweep', 'sweaty', 'sweats', 'swans', 'surprisingly', 'supply', 'superman', 'sup', 'sunshine', 'sun', 'sum', 'sulk', 'suite', 'suggestions', 'sue', 'sucker', 'subtle', 'style', 'stunning', 'stuffing', 'studied', 'stryker', 'stronger', 'strips', 'strings', 'strangers', 'strange', 'stores', 'storage', 'stood', 'stolen', 'stock', 'sticking', 'stevie', 'steak', 'stays', 'stayed', 'staten', 'starving', 'spots', 'spooky', 'spleen', 'spirit', 'spelled', 'specially', 'speaking', 'speacial', 'spat', 'spaceship', 'south', 'source', 'sophisticated', 'solid', 'soft', 'soak', 'snow', 'slower', 'slight', 'slap', 'sky', 'skipped', 'skip', 'skating', 'skates', 'sixteen', 'sissy', 'singing', 'simmons', 'signed', 'sidecar', 'sid', 'shrill', 'shrieking', 'shots', 'shorts', 'shirts', 'ship', 'shhh', 'she’s', 'sheer', 'shake', 'shadow', 'seventh', 'settled', 'setting', 'series', 'sensed', 'secure', 'seats', 'screwing', 'screaming', 'scott', 'score', 'scorcese', 'scooch', 'scenes', 'satin', 'sat', 'sangria', 'sanders', 'salty', 'salt', 'saliva', 'saint', 'sailing', 'rushed', 'rush', 'running', 'round', 'rough', 'rossy', 'ronni', 'rolls', 'role', 'rocks', 'robe', 'roast', 'river', 'ringing', 'revenge', 'retainer', 'restaurants', 'responsible', 'respectfully', 'resentment', 'reputation', 'represents', 'reporter', 'replace', 'repeating', 'rented', 'remembering', 'relieved', 'relationships', 'rejoin', 'regatta', 'refuse', 'refrigerator', 'refill', 'recently', 'recent', 'realistic', 'realised', 'reacted', 'react', 'reach', 're', 'rapist', 'rang', 'ramoray', 'rainy', 'raining', 'radioactive', 'racist', 'quiet', 'questioning', 'queens', 'qualified', 'puzzles', 'puzzle', 'pushover', 'puppets', 'punk', 'pumpkin', 'puff', 'published', 'proves', 'prove', 'proposed', 'propose', 'property', 'prom', 'progressionist', 'pretzel', 'prepared', 'precious', 'pre', 'practicing', 'pottery', 'potential', 'potatoes', 'potato', 'possibility', 'popcorn', 'police', 'pointing', 'poet', 'plural', 'pledges', 'pleasant', 'players', 'plates', 'planetarium', 'plains', 'pl', 'pipe', 'pinned', 'pin', 'pillow', 'pill', 'pile', 'picnic', 'picks', 'pickles', 'phrase', 'photos', 'phonetically', 'petrie', 'peter', 'pet', 'permanent', 'perk', 'period', 'performance', 'penis', 'pen', 'pees', 'peep', 'peas', 'paulo', 'patrick', 'patience', 'patch', 'paste', 'pas', 'partying', 'paranoid', 'parade', 'panicked', 'pan', 'paints', 'painful', 'owes', 'overseas', 'oughta', 'ordinary', 'opossum', 'oop', 'ooohh', 'onstage', 'onions', 'ole', 'oi', 'ohhhhh', 'officially', 'officer', 'offer', 'observe', 'oath', 'nutmeg', 'nut', 'nurses', 'nuh', 'nuclear', 'nowhere', 'northern', 'ninth', 'nighttime', 'nicest', 'neutral', 'nelson', 'neil', 'neighborhood', 'naps', 'naples', 'nanny', 'nancy', 'nakedness', 'nails', 'nailed', 'na', 'musta', 'mushroom', 'movement', 'mouthing', 'moustache', 'mousse', 'motion', 'mostly', 'moron', 'morgan', 'mores', 'monopoly', 'monogamy', 'monkeys', 'monet', 'monana', 'moms', 'momma', 'mocking', 'mmmm', 'mistakes', 'mississ', 'mirror', 'miracle', 'mint', 'miller', 'mill', 'milan', 'mia', 'mexico', 'method', 'messing', 'messages', 'mentioned', 'mentally', 'memory', 'memorize', 'memorial', 'member', 'mel', 'meaning', 'meal', 'max', 'mattress', 'matters', 'maternity', 'material', 'massaging', 'massages', 'massaged', 'masculine', 'martin', 'marrying', 'maria', 'marge', 'manly', 'maniac', 'manhattan', 'manage', 'main', 'magician', 'magical', 'luggage', 'luckiest', 'low', 'loving', 'loveable', 'lounge', 'louder', 'lotion', 'los', 'lorraine', 'lookin', 'lobster', 'lobby', 'loan', 'lo', 'liquor', 'linen', 'lid', 'levels', 'level', 'lens', 'length', 'lecture', 'learning', 'layers', 'lay', 'latte', 'latour', 'lasted', 'lasagne', 'lasagna', 'las', 'lapsed', 'lap', 'language', 'landlord', 'lake', 'laid', 'ladyfingers', 'lads', 'label', 'lab', 'krog', 'krista', 'kori', 'knuckle', 'knives', 'knife', 'knick', 'kisser', 'kingdom', 'killer', 'keystone', 'kevin', 'kelly', 'katie', 'kate', 'karen', 'kaplan', 'k', 'ju', 'joy', 'josh', 'johnson', 'jogging', 'joanna', 'joan', 'jo', 'jingle', 'jew', 'jet', 'jerks', 'jennifer', 'jen', 'jazz', 'jar', 'japan', 'january', 'jackass', 'i’ve', 'italy', 'issues', 'irony', 'invitation', 'intuitive', 'intrigued', 'intravenous', 'intimidated', 'internet', 'intelligent', 'integrity', 'inspector', 'insecure', 'innocent', 'ink', 'ing', 'india', 'indeedy', 'impressive', 'impressions', 'importantly', 'impact', 'illinois', 'ignore', 'iced', 'iceberg', 'hurely', 'huntley', 'hums', 'howdy', 'hottest', 'hostess', 'horsing', 'horse', 'horoscope', 'hopping', 'hopeless', 'hook', 'hooing', 'honing', 'homework', 'hockey', 'hmmm', 'hip', 'hint', 'higher', 'hibachi', 'he’s', 'herself', 'herbal', 'helps', 'helped', 'helen', 'heights', 'heck', 'hating', 'hardest', 'happiness', 'handy', 'handles', 'hamper', 'hammer', 'hamm', 'hamburger', 'hairy', 'guts', 'gumball', 'guilty', 'guilt', 'guide', 'guests', 'groceries', 'gravy', 'grant', 'grandfather', 'graduated', 'grabs', 'government', 'gotcha', 'gorilla', 'goose', 'goods', 'goodnight', 'gogo', 'goalie', 'glen', 'girlfriends', 'gina', 'gettin', 'gentle', 'generous', 'general', 'gellers', 'funeral', 'fully', 'fulfilling', 'frozen', 'fourteen', 'forms', 'forgotten', 'forgetting', 'foosball', 'fooled', 'folks', 'flower', 'florida', 'flew', 'flattered', 'flanging', 'fixed', 'fits', 'fingers', 'fifty', 'fiancée', 'fiancee', 'feminist', 'fella', 'feature', 'feast', 'favour', 'faster', 'fascinated', 'fanny', 'fairly', 'failed', 'factory', 'factor', 'facility', 'fabulous', 'fabric', 'f', 'expression', 'exploded', 'explains', 'exotic', 'exact', 'ethan', 'error', 'err', 'epic', 'enter', 'english', 'emotional', 'embryos', 'embarrass', 'elevator', 'elaine', 'ehm', 'egypt', 'eggs', 'efficient', 'effective', 'eaten', 'earlier', 'ear', 'dunno', 'dumping', 'du', 'drunken', 'drum', 'drug', 'drowning', 'drove', 'drops', 'dropping', 'driving', 'dressing', 'dresser', 'draw', 'drank', 'drama', 'drag', 'doug', 'doubt', 'doofy', 'donated', 'donate', 'dogs', 'disturbing', 'distract', 'disposable', 'dishes', 'discipline', 'dirk', 'direction', 'dinosaurs', 'dingy', 'ding', 'dina', 'dilated', 'dies', 'devil', 'destroy', 'dessert', 'designers', 'designer', 'describe', 'depressing', 'depressed', 'deposit', 'depends', 'delicious', 'deli', 'define', 'defense', 'dealing', 'dances', 'dancers', 'dancer', 'daily', 'cutie', 'cuter', 'custard', 'cushion', 'cursed', 'cubicle', 'cube', 'cuba', 'crust', 'cruel', 'crossword', 'criticism', 'crib', 'create', 'crabby', 'crab', 'cotton', 'copies', 'cop', 'convince', 'convention', 'contract', 'contestants', 'constant', 'consider', 'conquered', 'connect', 'concentric', 'competitive', 'competition', 'companies', 'common', 'collins', 'coincidence', 'closest', 'clients', 'cleaner', 'cleaned', 'circus', 'cinderelly', 'cilantro', 'cigarettes', 'chubby', 'chris', 'choking', 'chocolates', 'chocolate', 'chippy', 'china', 'chin', 'chicks', 'chester', 'chest', 'cheshhh', 'chefs', 'cheer', 'cheap', 'chatracus', 'charm', 'charlie', 'charcoal', 'champion', 'champagne', 'chairs', 'ceremony', 'cereal', 'cell', 'celebrating', 'ceiling', 'cats', 'catholic', 'catering', 'casting', 'cartoon', 'cartons', 'carpet', 'carefully', 'captain', 'cappuccino', 'capable', 'cancelled', 'canceled', 'canadian', 'cakes', 'cable', 'buys', 'butter', 'busted', 'buried', 'burial', 'burger', 'bumpy', 'bumps', 'bumped', 'bump', 'bum', 'build', 'bruiser', 'brownies', 'brownie', 'broom', 'brooklyn', 'brochures', 'broadway', 'brings', 'brighten', 'bridge', 'breath', 'brag', 'bouree', 'bouillabaisse', 'bottle', 'bossy', 'bored', 'bookstore', 'boobies', 'bonehead', 'bone', 'boles', 'bloomingdale’s', 'bloomingdale', 'blast', 'blah', 'blades', 'birdie', 'bicep', 'betty', 'betting', 'bernice', 'benefits', 'belt', 'belly', 'behaved', 'beginning', 'beg', 'bedrooms', 'becker', 'beating', 'bearnaise', 'beans', 'batman', 'bath', 'basis', 'barrel', 'barley', 'barium', 'bananas', 'banana', 'balls', 'ballroom', 'balloon', 'balance', 'baking', 'bail', 'bagel', 'backstage', 'backpacking', 'bachelor', 'awfully', 'avenue', 'aurora', 'attack', 'atlantic', 'asteroids', 'assumed', 'askin', 'ashamed', 'armageddon', 'arcade', 'appointments', 'application', 'applaud', 'appears', 'apology', 'apologise', 'apollo', 'anytime', 'antique', 'annabelle', 'ankle', 'animals', 'animal', 'angel', 'andie', 'american', 'allergies', 'alike', 'alien', 'albany', 'aisle', 'aim', 'aid', 'ahhh', 'advantage', 'adult', 'adrienne', 'adore', 'adding', 'added', 'actress', 'abott', 'abort', 'abandoned', 'aaron', '93', '816', '76', '74', '717', '500', '48', '45', '41', '35', '33', '28', '23', '22', '18', '15s', '1000', '”', 'zygomatic', 'zip', 'zinfandel', 'zine', 'zima', 'zillionaire', 'zero', 'zelda', 'yummy', 'yum', 'yugoslavian', 'yowza', 'you’ll', 'youngest', 'yoo', 'yolk', 'yeti', 'yesteryear', 'yesss', 'yertle', 'yentel', 'yellow', 'yearly', 'yeaaah', 'yea', 'yarns', 'yard', 'xerox', 'wynona', 'wxrk', 'wussies', 'wth', 'wry', 'writer', 'wrinkled', 'wrestle', 'wracking', 'wowww', 'wore', 'woooo', 'wooo', 'woolery', 'wong', 'wonerful', 'wonderment', 'wonderfulness', 'womba', 'wolves', 'wo', 'witnesses', 'within', 'withdrawal', 'wishes', 'wisely', 'wisecracking', 'wisdom', 'wires', 'wired', 'wiped', 'wintergarden', 'winterberry', 'winter', 'wins', 'winona', 'wink', 'wing', 'windshield', 'windscreen', 'willing', 'willie', 'willick', 'william', 'wildly', 'wildlife', 'wigging', 'widower', 'wide', 'whooooaaaa', 'whoooaa', 'wholesale', 'whoah', 'whoaa', 'whisper', 'whippin', 'whining', 'whiff', 'wheeler', 'wheeled', 'wheelchair', 'whazzup', 'what’s', 'whatcha', 'whaddya', 'we’re', 'we’ll', 'weston', 'westmont', 'westburg', 'west', 'wenus', 'weights', 'weepers', 'weekly', 'weeeell', 'wee', 'wednesday', 'wedgie', 'websites', 'website', 'weave', 'wears', 'weapon', 'waxing', 'waved', 'waters', 'waterfalls', 'watched', 'wasted', 'washington', 'washed', 'warrant', 'warning', 'warned', 'warn', 'wank', 'wander', 'wallpaper', 'wallets', 'wallet', 'walks', 'waitresses', 'waitin', 'waisted', 'wagering', 'waffles', 'waah', 'vulnerable', 'vulnerability', 'vstbl', 'vous', 'voulez', 'voor', 'vomiting', 'volcano', 'voices', 'vodka', 'vividly', 'visits', 'visited', 'visa', 'virgo', 'violent', 'violation', 'vincent', 'vikings', 'viking', 'vienna', 'videos', 'victorian', 'victoria', 'victim', 'vice', 'vicar', 'via', 'vh', 'vet', 'vessel', 'version', 'vermont', 'verbal', 'velula', 'veil', 'vcr', 'varrrrrrrrrrom', 'varrrrrrrrroom', 'varrrrrroom', 'varnish', 'vap', 'valuable', 'valid', 'valet', 'valente', 'val', 'vail', 'vague', 'vaginal', 'vacation', 'usual', 'usher', 'uses', 'user', 'useless', 'urine', 'uptown', 'upstate', 'upstage', 'ups', 'update', 'unyielding', 'unwitting', 'unsuccessful', 'unsettling', 'unpopular', 'unpacked', 'unnecessary', 'unnatural', 'unmarriable', 'unlikely', 'united', 'unisex', 'unicorn', 'unfulfilling', 'unexpected', 'unearthed', 'undress', 'undo', 'understood', 'understaffed', 'underneath', 'underlined', 'uncoordinated', 'uncooked', 'unconsciously', 'unclogging', 'unbound', 'unbearable', 'unattractive', 'unable', 'ummmmmmmm', 'ummm', 'ultimately', 'uhhhh', 'ugliest', 'uff', 'uck', 'typographical', 'typing', 'tyler', 'tuxedos', 'tuttle', 'tutti', 'tuschy', 'turtle', 'turkeys', 'tunnel', 'tunel', 'tune', 'tummy', 'tulips', 'tulip', 'tucked', 'tryin', 'trusts', 'trunk', 'trump', 'trudie', 'trucks', 'trrrribbiani', 'trppd', 'trousers', 'trorro', 'tron', 'triumph', 'triskaidekaphobia', 'triscuts', 'trips', 'tripped', 'triplets', 'tricky', 'tricks', 'trickling', 'trib', 'trial', 'trench', 'trebek', 'treating', 'trays', 'tray', 'travelling', 'traveled', 'traumatised', 'trap', 'translation', 'transition', 'transit', 'transferring', 'transferred', 'trampled', 'trains', 'trainer', 'trained', 'traditional', 'tradition', 'trading', 'tracks', 'towels', 'toward', 'tow', 'tov', 'tournaments', 'tournament', 'tourists', 'toulouse', 'toughest', 'touchy', 'toss', 'torture', 'tornado', 'torn', 'torme', 'toots', 'toothpicks', 'toothless', 'tooth', 'tone', 'tomorow', 'tommyyyy', 'tommy', 'tolouse', 'toll', 'toiletries', 'toga', 'tofu', 'toe', 'tissues', 'tissue', 'tipped', 'tinted', 'tingly', 'timing', 'time’s', 'timer', 'till', 'tiki', 'tiger', 'tide', 'tickled', 'tibidaybo', 'throws', 'throat', 'thrice', 'thousands', 'thorough', 'thompson', 'thomas', 'thirties', 'thigh', 'thieves', 'thick', 'thetoughest', 'theta', 'therapy', 'theoretically', 'theirs', 'thata', 'thanx', 'thanksgivings', 'thankful', 'thai', 'testosteroney', 'terrified', 'term', 'tension', 'tense', 'tennille', 'tender', 'tempted', 'temporary', 'temporarily', 'temporal', 'teller', 'teflon', 'teeny', 'tedlock', 'teddy', 'ted', 'technology', 'technical', 'tears', 'teamwork', 'teams', 'teachin', 'taxi', 'tax', 'taurus', 'tattoos', 'tasty', 'taping', 'tanned', 'tango', 'tampons', 'tallies', 'talky', 'talker', 'talented', 'tale', 'taj', 'tadpoles', 'taco', 'tabs', 'taboo', 'tabatha', 'tab', 'ta', 'szechwan', 'syracuse', 'syphilis', 'syndrome', 'swore', 'sword', 'switzerland', 'swirl', 'swimmers', 'swift', 'sweets', 'sweepin', 'swedish', 'sweatpants', 'sweating', 'swamped', 'swam', 'swallow', 'suzie', 'suspects', 'sushi', 'survived', 'surrounded', 'surrogate', 'surrogacy', 'surgeons', 'surefire', 'sur', 'supporting', 'supported', 'superstar', 'superiors', 'superficial', 'superb', 'super', 'sunrise', 'sunglasses', 'sung', 'sundays', 'sunburned', 'summertime', 'sullies', 'sulkov', 'suity', 'suited', 'suitcases', 'suicide', 'suggested', 'suggest', 'sucking', 'suckers', 'successful', 'substantial', 'substances', 'subscription', 'subletting', 'subject', 'styles', 'sturdy', 'sturdier', 'stupider', 'stunted', 'stung', 'stumble', 'stuffers', 'studying', 'studies', 'stuart', 'stu', 'stroller', 'stroll', 'stroke', 'strippers', 'string', 'strict', 'stretching', 'stressed', 'streisand', 'streetlight', 'strapping', 'strain', 'straightener', 'straighten', 'storming', 'stopping', 'stoop', 'stomping', 'stocked', 'stitches', 'stirring', 'stinkers', 'stink', 'stimulate', 'stewardess', 'stereo', 'steps', 'stepped', 'stephanie', 'stepfather', 'steered', 'steel', 'steam', 'status', 'statue', 'statistical', 'stating', 'startin', 'starring', 'starlight', 'stares', 'stanley', 'stamp', 'stalk', 'stairway', 'stairs', 'stains', 'staff', 'stability', 'squeezed', 'squeaky', 'squatternut', 'squat', 'squash', 'spying', 'spurred', 'spun', 'sprinkle', 'spring', 'spreading', 'spread', 'spraying', 'spotted', 'spontaneous', 'spoken', 'spoil', 'spleeeen', 'spitter', 'spins', 'spinning', 'spindler', 'spin', 'spice', 'spends', 'spelt', 'spell', 'spectacular', 'specimen', 'specified', 'specifically', 'species', 'speaks', 'speakers', 'speaker', 'spatters', 'sparkly', 'spandex', 'spaghetti', 'spackle', 'spackel', 'sowed', 'souvenirs', 'southern', 'sour', 'soundtrack', 'sounded', 'soul', 'sorts', 'sore', 'sooner', 'sonoma', 'sonogram', 'sonic', 'somehow', 'solved', 'solve', 'solution', 'solidify', 'solider', 'soir', 'softener', 'softball', 'sofa', 'socks', 'socket', 'soccer', 'soaps', 'soaked', 'snuggles', 'snuggle', 'snug', 'snuffalopagus', 'snort', 'snooty', 'snoopy', 'sneezes', 'sneezed', 'sneaking', 'smokey', 'smoker', 'smock', 'smiledon', 'smeared', 'smarter', 'smacks', 'smack', 'slutty', 'slots', 'sloppy', 'sliver', 'slips', 'slipped', 'slimmer', 'sleigh', 'sleeve', 'sleepy', 'sleeperson', 'sleeper', 'slays', 'slaw', 'slash', 'slammer', 'skull', 'skimp', 'skidmark', 'skedaddle', 'skaters', 'sized', 'sixes', 'sincerely', 'simple', 'simp', 'simon', 'silk', 'silently', 'silence', 'sight', 'sidewalk', 'sidestep', 'sides', 'sidekick', 'shuttle', 'shutdown', 'shuffleboard', 'shrink', 'shrewdly', 'showtime', 'shows', 'showroom', 'showers', 'showering', 'shove', 'shoulder', 'shoulda', 'shorter', 'shorten', 'shortbread', 'shoppin', 'shooting', 'shootin', 'shook', 'shocks', 'shmair', 'shirvel', 'ships', 'shiny', 'shining', 'shin', 'shift', 'shielding', 'shhhh', 'shelter', 'shelley', 'shedder', 'shed', 'shaving', 'shark', 'shared', 'shampoos', 'shampoo', 'shallots', 'shall', 'shakin', 'sexuality', 'sexless', 'seventeen', 'settle', 'sets', 'setback', 'session', 'sesame', 'serves', 'servants', 'sergio', 'sentence', 'sensing', 'senior', 'sends', 'sending', 'seminar', 'semen', 'seller', 'selfish', 'self', 'seeking', 'seeeeen', 'sections', 'secretly', 'secluded', 'seating', 'seasons', 'seashores', 'seamstress', 'seal', 'seahorses', 'seahorse', 'se', 'sculpture', 'scrunchy', 'scrum', 'scrub', 'screening', 'screen', 'screams', 'screamed', 'scratch', 'scouts', 'scotty', 'scotland', 'scotia', 'scores', 'scored', 'scooter', 'scooching', 'scientists', 'schools', 'schoolchildren', 'scholars', 'schmoon', 'schemp', 'scheduled', 'scarves', 'scars', 'scaring', 'scarf', 'scares', 'scans', 'scale', 'savory', 'savings', 'sautéed', 'saucy', 'saturn', 'satisfy', 'satan', 'sarandon', 'santos', 'sandra', 'sand', 'sanctity', 'samples', 'sam', 'salute', 'sales', 'salami', 'sajak', 'saj', 'sailor', 'sailed', 'sail', 'sagittarius', 'sage', 'safety', 'sadly', 'saddest', 'sacramental', 'sabotage', 'ryder', 'ryan', 'ruth', 'rustic', 'russell', 'rushing', 'ruptured', 'runnin', 'rumors', 'rum', 'rue', 'ruben', 'rubber', 'rub', 'rsvp', 'rrific', 'rosten', 'rose', 'rope', 'roots', 'roomie', 'rookie', 'rolling', 'roller', 'rolled', 'rodney', 'rode', 'rod', 'rockefeller', 'rocked', 'robots', 'robbie', 'ritual', 'ritter', 'risky', 'risked', 'risk', 'ringside', 'righty', 'rights', 'righteous', 'riggs', 'ridiculously', 'ridicule', 'rides', 'rider', 'rick', 'richie', 'rice', 'rhythm', 'rhyming', 'rhymes', 'revolutionary', 'revolution', 'revoked', 'reviewer', 'reverse', 'returning', 'retiling', 'rethink', 'resumes', 'resume', 'restrooms', 'restrictions', 'restricted', 'restraining', 'resting', 'response', 'respond', 'respects', 'respected', 'resist', 'residual', 'resident', 'reserve', 'reservation', 'rescue', 'requires', 'required', 'requesting', 'replica', 'repel', 'repeated', 'repeat', 'renewed', 'renaissance', 'removal', 'remotely', 'reminiscing', 'remedies', 'reliving', 'relish', 'relief', 'release', 'relatives', 'relative', 'rejections', 'rejection', 'rejecting', 'reinforcements', 'rehearsed', 'regulations', 'regularly', 'regret', 'register', 'regina', 'referring', 'refer', 'reduction', 'redo', 'rediscover', 'redecorate', 'recover', 'record', 'reconfiguration', 'recommended', 'recognize', 'recognise', 'reciprocate', 'receptionists', 'reception', 'receive', 'receipt', 'recall', 'rec', 'rebuilding', 'reattach', 'reassure', 'rear', 'reality', 'realises', 'realilized', 'reaffirms', 'reads', 'readers', 'reactor', 'reaction', 'reacquainted', 'reaching', 'rdtor', 'razor', 'ravioli', 'rattraps', 'rat', 'raspberries', 'rare', 'raquetball', 'rapidly', 'rape', 'rap', 'ranger', 'randy', 'random', 'rambunctious', 'rally', 'raising', 'raisin', 'raised', 'rainstorm', 'rage', 'radiator', 'rack', 'racecar', 'rabbits', 'rabbit', 'quicker', 'qualifications', 'q', 'pyramid', 'pyjama', 'puzzler', 'puts', 'purses', 'purred', 'purchasing', 'puppies', 'punishment', 'punish', 'puncture', 'punctuated', 'punches', 'punched', 'pulse', 'puke', 'pudding', 'psychic', 'ps', 'pry', 'proved', 'prototypes', 'protects', 'protective', 'protecting', 'protected', 'proposing', 'proposal', 'pronouncing', 'projectile', 'progress', 'programming', 'profit', 'profile', 'producers', 'processing', 'process', 'prize', 'prison', 'printed', 'principle', 'pricing', 'prices', 'prettiest', 'presumptuous', 'pressuring', 'pressing', 'presenting', 'prescriptions', 'preppy', 'preparing', 'prepare', 'preference', 'precision', 'prank', 'prance', 'practical', 'pox', 'pouring', 'pounds', 'posture', 'postponing', 'posters', 'poster', 'possom', 'positive', 'positions', 'position', 'pose', 'portuguese', 'portraits', 'portion', 'porsching', 'porsch', 'pops', 'poppers', 'pooper', 'pools', 'pony', 'politics', 'political', 'polish', 'pole', 'poke', 'poisoning', 'poison', 'poetry', 'poems', 'plunky', 'plunge', 'plinky', 'pleased', 'plaza', 'plays', 'playfully', 'player', 'playboys', 'playboy', 'platonic', 'platforms', 'platform', 'planner', 'plain', 'plaid', 'plague', 'pizzas', 'pitfalls', 'pirate', 'pinecones', 'pine', 'pinching', 'pinchable', 'pillows', 'pillman', 'pillleeeee', 'pier', 'picturing', 'pictionary', 'picky', 'pickle', 'pi', 'physically', 'phones', 'phoned', 'phobo', 'phobia', 'philly', 'phillips', 'philange', 'philadelphia', 'phial', 'phewbedo', 'phenomenon', 'phenomenal', 'phd', 'phaybobo', 'phasing', 'pharmacists', 'pharmaceutical', 'ph', 'pfieffer', 'petty', 'pets', 'petition', 'petite', 'pesos', 'pervert', 'personally', 'personalities', 'perps', 'perp', 'permitted', 'perfume', 'perfectly', 'perceptive', 'per', 'pepponi', 'pepper', 'pep', 'people’s', 'penny', 'pencil', 'penache', 'pelvis', 'peeper', 'peek', 'peed', 'pedals', 'pear', 'peacock', 'peaches', 'peaceful', 'peace', 'pbs', 'pays', 'payin', 'payback', 'paulette', 'patio', 'paths', 'patches', 'pastry', 'passionately', 'passionate', 'passion', 'passing', 'passin', 'passed', 'passageway', 'parts', 'partners', 'partly', 'parties', 'particular', 'participate', 'parking', 'parentteacher', 'parent', 'parallel', 'parachute', 'papers', 'panties', 'pals', 'paleontologists', 'paintings', 'painterly', 'painted', 'paint', 'packing', 'packed', 'packaging', 'pacino', 'pacifist', 'pace', 'pac', 'pa', 'owner', 'owl', 'overweight', 'overview', 'overslept', 'overpriced', 'overnight', 'overheard', 'overdue', 'overcame', 'overall', 'ovens', 'oven', 'outing', 'outer', 'outdoorsy', 'outdoors', 'ouch', 'others', 'ot', 'orrr', 'ornate', 'originated', 'original', 'orgies', 'organized', 'organize', 'organic', 'oreos', 'oregano', 'ordinarily', 'ordained', 'orbit', 'oranges', 'orange', 'orally', 'oral', 'option', 'opportunities', 'opponent', 'opium', 'opinions', 'opinion', 'operators', 'operation', 'opens', 'oops', 'oooooooooooooohhhhhhhhhhh', 'oooooo', 'ooooohh', 'oooooh', 'oooo', 'ooohhhh', 'ooo', 'oon', 'oohh', 'onto', 'onemillionth', 'olè', 'olympic', 'olivia', 'oldest', 'older', 'okayyyyy', 'ohhhhhhhh', 'ohhhh', 'og', 'offs', 'offered', 'offensive', 'offense', 'odour', 'octagon', 'occurred', 'occur', 'occasionally', 'occasional', 'obstacles', 'obsessive', 'obsessed', 'observed', 'obligation', 'objects', 'objectively', 'oak', 'nyu', 'nurturing', 'nursing', 'numb', 'nudity', 'nudes', 'november', 'nova', 'nouse', 'notices', 'nostril', 'normally', 'nora', 'nor', 'nooooooo', 'noooooo', 'noon', 'noodles', 'nonsense', 'nominees', 'nominated', 'nokululu', 'noisy', 'nodded', 'nobel', 'nnnnn', 'nngghhh', 'ninja', 'ninety', 'nineteenth', 'nineteen', 'nimitz', 'nightstand', 'nightmare', 'nighties', 'nicknames', 'nickel', 'nick', 'nicely', 'ni', 'newton', 'newspapers', 'newspaper', 'nevermind', 'neurologist', 'neurolic', 'nether', 'nestley', 'nestle', 'ness', 'nesele', 'nerves', 'nephew', 'neighbourhood', 'neighbor', 'needles', 'needing', 'nectarines', 'naughty', 'nature', 'naturally', 'national', 'nathan', 'nasty', 'napping', 'napkin', 'namesake', 'myth', 'mwwwooooo', 'mute', 'musings', 'musicman', 'musicians', 'musical', 'museums', 'muse', 'muscle', 'murray', 'muriel', 'murder', 'munchies', 'mumbles', 'mumble', 'mum', 'multiplex', 'multinational', 'muhawa', 'mugsy', 'mugs', 'mug', 'muffin', 'mtv', 'mri', 'mozzarella', 'mouthful', 'mount', 'motorcycle', 'motility', 'moskowitz', 'mortician', 'morse', 'moral', 'mopping', 'mop', 'moonlit', 'moonlight', 'montauk', 'monster', 'monroe', 'monocle', 'monnnnn', 'monicas', 'monger', 'mommy', 'mommies', 'moments', 'moisturizer', 'moist', 'moi', 'moderation', 'modeling', 'model', 'mixed', 'mitzvah', 'misunderstand', 'mistress', 'mister', 'mistaken', 'missy', 'mississippi', 'mission', 'missin', 'misses', 'misery', 'miserable', 'mischievous', 'mischa', 'mirrored', 'mira', 'mints', 'minor', 'minks', 'minimal', 'mingle', 'minds', 'mime', 'milwaukee', 'millionaire', 'millennium', 'miles', 'mikey', 'mignon', 'midterms', 'midterm', 'mid', 'microphones', 'michelle', 'metaphor', 'metal', 'mesozoic', 'mesopotamia', 'merry', 'mere', 'mercy', 'mercial', 'menu', 'mentor', 'mental', 'ment', 'mena', 'memos', 'memorized', 'memo', 'members', 'mellow', 'meg', 'meets', 'mediterranean', 'mediocre', 'medieval', 'medicine', 'medeio', 'meddled', 'mechanism', 'mechanical', 'meat', 'measures', 'meanwhile', 'meantime', 'meaningless', 'meals', 'meadow', 'mcpretty', 'mcnails', 'mcguire', 'mcclane', 'mazel', 'mayor', 'maurice', 'maturity', 'mature', 'mattresses', 'mattered', 'matt', 'mathematician', 'math', 'mate', 'matches', 'matador', 'massager', 'mask', 'mashuga', 'masculinity', 'mascot', 'marvel', 'martini', 'marshmellows', 'marquel', 'maroon', 'markson', 'marine', 'marinara', 'marijuana', 'margaritas', 'marcia', 'march', 'maple', 'manuver', 'manuscript', 'manual', 'mannequins', 'mannequin', 'manipulative', 'manhood', 'mango', 'mama', 'mally', 'mall', 'maintenance', 'mahogany', 'mahal', 'magnificent', 'magazine', 'madly', 'madison', 'macy', 'macpherson', 'machines', 'macaroons', 'macaroni', 'maaaaadd', 'lyin', 'lycra', 'luxuries', 'lusts', 'lust', 'lurker', 'lure', 'lunges', 'lung', 'lumps', 'lumberjack', 'luckily', 'luckier', 'luchhi', 'loyal', 'lovable', 'lousy', 'lotta', 'loss', 'loses', 'loreo', 'lord', 'looters', 'loosened', 'loosen', 'lodged', 'locker', 'locations', 'lobbing', 'loaner', 'loaned', 'load', 'lizzie', 'litman', 'lit', 'listings', 'listens', 'listener', 'lisettie', 'lisbon', 'lint', 'lingerie', 'ling', 'linda', 'limited', 'limb', 'lima', 'lightning', 'lighting', 'lifting', 'lifeguards', 'lickers', 'licence', 'libs', 'liberty', 'libbing', 'liar', 'liaisons', 'let’s', 'letterman', 'lest', 'leroy', 'leprechaun', 'leon', 'lent', 'lenny', 'lee', 'led', 'lebanon', 'leaves', 'leathery', 'learners', 'leap', 'leaning', 'leaned', 'leakey', 'leak', 'leaf', 'leading', 'layman', 'laying', 'layering', 'lawyers', 'laughter', 'laughs', 'lauer', 'lasts', 'laser', 'lane', 'landed', 'lamps', 'lame', 'lambs', 'lag', 'lafite', 'ladybugs', 'lacys', 'lace', 'labels', 'l', 'kurt', 'kostelick', 'koon', 'koffie', 'knitting', 'knit', 'knees', 'knapsack', 'knack', 'kleinman', 'kl', 'kitten', 'kissin', 'kissey', 'kings', 'kindergarten', 'kilmer', 'kiev', 'kidney', 'kiddy', 'kicky', 'keynote', 'keyboard', 'kettle', 'keepers', 'keeper', 'keaton', 'kane', 'jurassic', 'junk', 'juicy', 'judy', 'judgmental', 'judging', 'judgey', 'journal', 'josephine', 'joked', 'johnos', 'joel', 'jody', 'jockstrap', 'jobs', 'joanne', 'jll', 'jinx', 'jews', 'jewish', 'jewelry', 'jewellers', 'jetway', 'jester', 'jersey', 'jeffrey', 'jeez', 'jaw', 'java', 'jared', 'japanese', 'janet', 'jammies', 'jalepino', 'jackets', 'j437', 'i’ll', 'ity', 'item', 'itches', 'ish', 'irreplaceable', 'iron', 'irene', 'iran', 'involved', 'involve', 'invisible', 'invest', 'invasive', 'invaders', 'introduction', 'introducing', 'intimated', 'interviews', 'interviewing', 'interviewed', 'interruptions', 'interrupting', 'interfacing', 'interests', 'interestingly', 'intercourse', 'interactive', 'intently', 'intended', 'intelligence', 'intellectual', 'instructor', 'instinct', 'instance', 'inspectors', 'insignificant', 'insemination', 'insects', 'innate', 'injure', 'injection', 'initiated', 'initials', 'inhospitable', 'informed', 'info', 'influence', 'inflicting', 'inflict', 'inflate', 'inflatable', 'indians', 'independent', 'indeed', 'increasing', 'increase', 'inconvenience', 'incompetent', 'inch', 'impulsively', 'impressed', 'importance', 'implementing', 'implantation', 'impasto', 'immodest', 'immature', 'imax', 'imagining', 'imagined', 'imagination', 'imaginary', 'images', 'illegally', 'ill', 'iffy', 'idiots', 'ideally', 'id', 'icky', 'hyphen', 'hygienist', 'hygiene', 'hydrosaurs', 'hydrosaurids', 'hurtin', 'hunting', 'hunger', 'hundreds', 'humping', 'humidity', 'hum', 'hulk', 'hugsy', 'hugs', 'hubba', 'houses', 'hostages', 'hoshi', 'horror', 'horribly', 'hopes', 'hop', 'hoops', 'hoop', 'hoooo', 'hooohhh', 'hookin', 'homosexually', 'homo', 'hometowns', 'homesick', 'homeless', 'holyfield', 'holocaust', 'hollandaise', 'holierthanthou', 'holes', 'hmph', 'hitchhiker', 'hiring', 'hire', 'hips', 'hill', 'hilda', 'hilarious', 'hike', 'highway', 'highness', 'highly', 'highlight', 'hiding', 'hickey', 'hhiii', 'hewitt', 'heroin', 'heritage', 'herd', 'helpful', 'helper', 'held', 'hel', 'hef', 'heels', 'heel', 'heavy', 'heating', 'hearts', 'heals', 'headrest', 'headed', 'hbo', 'hazel', 'hazard', 'haunt', 'haul', 'hassidic', 'hardy', 'hardwood', 'hardware', 'harassment', 'happier', 'hanukkah', 'hanks', 'hangs', 'hangover', 'hanger', 'handyman', 'handshake', 'handing', 'handed', 'handbook', 'handbag', 'hammered', 'hammel', 'hamilton', 'hallway', 'halloween', 'hairier', 'hairdo', 'haircut', 'haha', 'hadens', 'haa', 'guysll', 'guysd', 'guru', 'gunter', 'guns', 'guitars', 'guild', 'guessing', 'guarantees', 'guarantee', 'groupie', 'grounds', 'gross', 'grope', 'groin', 'grip', 'grin', 'grilled', 'grieve', 'grief', 'grey', 'greeting', 'greenpeace', 'greely', 'greedy', 'greasy', 'grazie', 'gravity', 'grath', 'grasp', 'grappa', 'grape', 'granted', 'granddaughter', 'grand', 'graham', 'graduation', 'graduate', 'graceful', 'grace', 'gr', 'gown', 'gouging', 'gouged', 'gossiping', 'gossip', 'gooooood', 'google', 'goofy', 'goofin', 'goof', 'goody’s', 'gonzalez', 'goldie', 'golden', 'goind', 'gods', 'goblins', 'gobb', 'goat', 'goal', 'glued', 'glue', 'gloves', 'globe', 'glenda', 'glee', 'glaucoma', 'glasses', 'glare', 'glance', 'gist', 'girlish', 'girlie', 'giraffe', 'giorno', 'gimmie', 'gifts', 'gibson', 'ghosts', 'gesturing', 'gerston', 'gepeto', 'geography', 'geographically', 'genuinely', 'gently', 'gentleman', 'genitals', 'genie', 'generations', 'generally', 'gemini', 'gellar', 'gel', 'geeky', 'geeks', 'gdcr', 'gaze', 'gawk', 'gawd', 'gathered', 'gather', 'gassy', 'garden', 'gar', 'gangster', 'gallon', 'gains', 'gained', 'gail', 'fuzzy', 'further', 'furnished', 'funyuns', 'funnily', 'fundamentally', 'functions', 'fulfilled', 'fulfill', 'fuels', 'fuel', 'frustrating', 'frowned', 'frown', 'frontal', 'frisbee', 'fringe', 'frightening', 'frieze', 'friend’s', 'friendship', 'fried', 'freshly', 'fresh', 'fresca', 'freemont', 'freeman', 'freckles', 'freaks', 'freakish', 'franzblau', 'francisco', 'fran', 'foxy', 'foxtrot', 'foxhole', 'fox', 'fountains', 'foundation', 'foul', 'foster', 'fossil', 'fortune', 'fort', 'formerly', 'form', 'forking', 'fork', 'forgiving', 'forgets', 'forehead', 'ford', 'forcing', 'foothills', 'fooling', 'fonzie', 'fond', 'folders', 'focusing', 'foam', 'flush', 'fluorescent', 'flung', 'fluids', 'fluid', 'fluff', 'floyd', 'flown', 'flowing', 'flow', 'floss', 'florist', 'flopping', 'floors', 'floopy', 'flood', 'flirting', 'flirt', 'flingy', 'flinging', 'flingin', 'flies', 'flick', 'flexible', 'flexed', 'flennin', 'flaws', 'flaw', 'flattery', 'flattering', 'flashback', 'flaky', 'fixin', 'fixable', 'fitting', 'fishhook', 'fisher', 'fished', 'firm', 'fires', 'firemen', 'finishing', 'finest', 'finding', 'finders', 'finalists', 'filter', 'filling', 'filet', 'fierce', 'fiddler', 'fictitious', 'fiction', 'fiasco', 'fewer', 'fettuccini', 'fertility', 'ferry', 'ferrets', 'felon', 'fellas', 'felicity', 'federal', 'fed', 'features', 'feasted', 'feasible', 'fears', 'favored', 'fatherhood', 'farrell', 'fares', 'fantasies', 'fans', 'fangs', 'fangled', 'fame', 'falls', 'fallin', 'fallen', 'faithful', 'faith', 'fairy', 'faint', 'fail', 'faeces', 'faded', 'fade', 'faculty', 'factoring', 'faced', 'fabutec', 'fabrics', 'ezels', 'eyelash', 'eyed', 'eyebrow', 'extremely', 'extras', 'expressly', 'express', 'exposed', 'explanation', 'expired', 'expert', 'experiment', 'expenses', 'expense', 'exited', 'executor', 'execute', 'exclusive', 'exchange', 'excedrin', 'example', 'exam', 'ewwuck', 'ewing', 'evicted', 'event', 'eve', 'evander', 'evaluate', 'eva', 'european', 'euphoria', 'ette', 'ethnic', 'eternity', 'estate', 'essential', 'essence', 'essay', 'espresso', 'escape', 'erwin', 'erotica', 'erotic', 'eric', 'erectus', 'eraser', 'er', 'equipped', 'equilibrium', 'episode', 'environment', 'envelope', 'entitled', 'entirely', 'enticed', 'entertain', 'entering', 'entered', 'enhh', 'enh', 'enemas', 'enema', 'ends', 'endless', 'endearing', 'emptier', 'emphysema', 'emotionally', 'emotion', 'embryossss', 'embassy', 'embarrassment', 'elusive', 'elton', 'elle', 'eliminated', 'electricity', 'electrician', 'electrical', 'electric', 'elastic', 'eighteenth', 'effort', 'effects', 'effect', 'effaced', 'eez', 'eel', 'ee', 'editing', 'edit', 'edgy', 'edges', 'edge', 'eddie', 'ec', 'eater', 'eastwood', 'easter', 'east', 'easiest', 'ease', 'earth', 'ears', 'earring', 'earl', 'dysfunctional', 'dynamic', 'dwha', 'duval', 'dutch', 'dungeon', 'dumbest', 'dull', 'duh', 'dug', 'dues', 'dudes', 'duchess', 'dubbies', 'drying', 'drumming', 'driver’s', 'driver', 'drift', 'dried', 'dressy', 'dresses', 'dreamless', 'dreamed', 'dreadful', 'drawings', 'drawer', 'drastic', 'drain', 'dragged', 'draft', 'draddle', 'dozens', 'downstage', 'download', 'dowdy', 'dove', 'douglas', 'dough', 'doubling', 'doubles', 'dots', 'dot', 'dorothy', 'dorms', 'dorfman', 'dora', 'doors', 'doorman', 'doorknobs', 'doooo', 'doogie', 'doodle', 'donuts', 'donut', 'donor', 'donna', 'donations', 'donald', 'don', 'domineering', 'dominance', 'dollywood', 'dokey', 'doin', 'doggy', 'doesn’t', 'doe', 'documents', 'docile', 'dna', 'divide', 'divert', 'diverse', 'dive', 'ditch', 'distribute', 'distraught', 'distance', 'disrupts', 'dismissed', 'dismiss', 'dismantle', 'disgustingtons', 'discussed', 'discuss', 'discoveries', 'discovered', 'discover', 'disconnect', 'disarray', 'disappointed', 'disappears', 'dirt', 'directly', 'directing', 'dinners', 'dining', 'diner', 'dime', 'dig', 'differ', 'diets', 'dick', 'dice', 'diaz', 'diaphragm', 'diane', 'dewey', 'devonian', 'devon', 'developed', 'devane', 'detail', 'destructive', 'destiny', 'destined', 'desire', 'designed', 'design', 'deserves', 'description', 'described', 'depths', 'depression', 'depending', 'dependent', 'dependant', 'dentists', 'dense', 'denise', 'denim', 'demeaning', 'delvecchio', 'deliveries', 'delivered', 'delightful', 'delicacy', 'dehydrated', 'definitive', 'definition', 'definite', 'deficit', 'defiantly', 'defend', 'declare', 'decimal', 'deciding', 'decides', 'decent', 'december', 'debbie', 'debating', 'deals', 'deaf', 'deadline', 'deadened', 'daytime', 'daydreaming', 'darts', 'darnit', 'darn', 'darlings', 'darling', 'dared', 'dangle', 'dangerous', 'danger', 'danced', 'dan', 'damaged', 'dakota', 'daiquiris', 'da', 'cycles', 'cuts', 'cutout', 'custom', 'cushions', 'curtain', 'curling', 'curler', 'curiously', 'curious', 'cupert', 'cupboard', 'culture', 'cultivating', 'culinary', 'cufflinks', 'cubby', 'cubans', 'crystal', 'crushing', 'crumpled', 'crumbly', 'crumbies', 'cruise', 'cruelty', 'crowning', 'crowd', 'croup', 'critics', 'crisp', 'crisis', 'cries', 'crew', 'credited', 'creamier', 'craziest', 'crawler', 'crawl', 'crashed', 'crash', 'crappy', 'cranky', 'crank', 'craniotomy', 'cramps', 'craftsmanship', 'cracks', 'cracker', 'cracked', 'craaazy', 'coyotes', 'cows', 'cowlicky', 'covers', 'coveralls', 'cousin', 'courtside', 'courthouse', 'courage', 'couples', 'coupla', 'countries', 'counting', 'counselor', 'couldn’t', 'coucher', 'costumes', 'costume', 'costs', 'costalano', 'cos', 'correctly', 'corporation', 'corporate', 'cornered', 'corner', 'corneas', 'corn', 'cord', 'copying', 'copperfield', 'cooper', 'cooked', 'convincing', 'convey', 'convenience', 'controversial', 'controlled', 'contribution', 'contracting', 'continuing', 'continue', 'contest', 'containers', 'contained', 'contact', 'constructive', 'construction', 'constantly', 'conquer', 'connected', 'congress', 'confusion', 'confused', 'confront', 'confidence', 'confidant', 'confess', 'conferences', 'cones', 'conducting', 'condom', 'condition', 'concussion', 'conclusion', 'concert', 'concentration', 'conceiving', 'computers', 'comprehensive', 'comprehends', 'compliment', 'complex', 'complete', 'complementary', 'complaints', 'complaint', 'complain', 'competitor', 'compartments', 'compared', 'compare', 'committee', 'committed', 'commission', 'commercials', 'comment', 'commando', 'comic', 'comfort', 'comedy', 'colours', 'coloured', 'colour', 'colors', 'colored', 'colorado', 'colonial', 'colonel', 'colon', 'cologne', 'collect', 'collapsed', 'cole', 'coin', 'coffin', 'code', 'cocoa', 'cocky', 'cocktails', 'cocktail', 'cockpit', 'coaching', 'coached', 'coach', 'clunked', 'clubbing', 'closets', 'closes', 'closely', 'clogging', 'clip', 'climbing', 'climb', 'clifford', 'cliff', 'cleveland', 'clerk', 'clergy', 'clearing', 'cleansing', 'cleaners', 'clawing', 'classroom', 'classics', 'classic', 'clark', 'clarify', 'clap', 'clamed', 'claims', 'claim', 'citizen', 'circumstances', 'circling', 'circle', 'cinemax', 'cinamon', 'cigs', 'cigar', 'cider', 'ciao', 'churo', 'chunk', 'chuckles', 'christmastime', 'christian', 'chou', 'chorus', 'chords', 'chord', 'choosing', 'choke', 'choices', 'chiropractor', 'chips', 'chipping', 'chipper', 'chipped', 'ching', 'chinese', 'chimney', 'chilliest', 'chilean', 'childhood', 'childbirth', 'chief', 'chicky', 'chickeeeen', 'chiaroscuro', 'chews', 'chewing', 'chewed', 'chew', 'chemistry', 'cheerie', 'cheered', 'cheeks', 'checkbook', 'cheating', 'cheated', 'cheat', 'charred', 'charles', 'charla', 'charisma', 'charging', 'characterize', 'channie', 'championship', 'champ', 'challenge', 'chalk', 'chad', 'cereals', 'celery', 'celebrities', 'celebration', 'ceilings', 'ce', 'ccan', 'cave', 'cathy', 'cathedral', 'caterpillars', 'caterer', 'category', 'casket', 'cash', 'cases', 'cartoons', 'carton', 'cart', 'carseat', 'cars', 'carryon', 'carried', 'carpel', 'carlos', 'carlin', 'carin', 'careless', 'careers', 'cardboard', 'carcass', 'carbon', 'caravaggio', 'carat', 'captured', 'capture', 'caplin', 'capades', 'capacity', 'cap', 'cantaloupe', 'cans', 'canned', 'candle', 'candels', 'cancer', 'canada', 'camper', 'camped', 'campaign', 'camp', 'cameron', 'calms', 'calming', 'callin', 'callbacks', 'californicus', 'calcutta', 'café', 'cafe', 'caf', 'caesar', 'cabinets', 'cabinet', 'byzantine', 'bwah', 'buzz', 'buying', 'buyer', 'buttons', 'buttery', 'butternut', 'butcher', 'busty', 'buster', 'bust', 'busiest', 'burying', 'burt', 'burst', 'burritos', 'burrito', 'burping', 'burnett', 'burgendy', 'burg', 'buns', 'bunnies', 'bums', 'bummer', 'bumb', 'bullets', 'bulging', 'bulb', 'bugger', 'budget', 'buddies', 'bubbling', 'bubbles', 'bubble', 'bubba', 'buash', 'bruise', 'brownshirt', 'brook', 'broiling', 'brilliant', 'bright', 'briefs', 'briefcase', 'brides', 'bridal', 'bricks', 'bribes', 'bribery', 'brenda', 'breeze', 'breaths', 'breadsticks', 'breadstick', 'brazilian', 'braverman', 'bras', 'brand', 'branching', 'branches', 'brains', 'brady', 'braces', 'brace', 'boys', 'bowmont', 'bouquet', 'bound', 'botchy', 'botched', 'bore', 'booked', 'boobie', 'bonus', 'bonking', 'bonfire', 'bones', 'bonding', 'bon', 'bologna', 'bobo', 'boats', 'blushed', 'blurtin', 'blur', 'blueberry', 'blows', 'blowout', 'blowing', 'blower', 'blouses', 'bloomingdales', 'bloomies', 'bloodbath', 'bloke', 'blocking', 'block', 'blobbies', 'blinding', 'blessing', 'bleeth', 'bleeding', 'bleaker', 'blaster', 'blarrglarrghh', 'blargon', 'blanket', 'blamed', 'blade', 'blacken', 'blacked', 'bitten', 'bitchin', 'bitches', 'bitched', 'binge', 'binding', 'bills', 'billfold', 'bigot', 'biggie', 'biceps', 'bicentennial', 'beyond', 'betraying', 'betray', 'beth', 'beta', 'beside', 'bernoulli', 'bernard', 'bergen', 'benefit', 'bend', 'bench', 'belongs', 'belong', 'bellyaching', 'belive', 'believes', 'believable', 'bela', 'bein', 'behaviour', 'behalf', 'begun', 'begins', 'beginners', 'beggin', 'bees', 'beers', 'beeps', 'bedside', 'bedankt', 'becoming', 'beaudalire', 'beats', 'bears', 'bearings', 'beardsley', 'bean', 'beakers', 'bazida', 'baywatch', 'battery', 'batter', 'bats', 'bathtub', 'baths', 'bathrooms', 'bat', 'bass', 'bask', 'basics', 'basic', 'bash', 'basel', 'baseboard', 'baseball', 'barracuda', 'barney', 'barber', 'barbara', 'baptism', 'banned', 'bang', 'bambi', 'balded', 'balcony', 'bakery', 'baked', 'bailed', 'baguette', 'bagpipes', 'bagina', 'bagels', 'badly', 'baddest', 'bactine', 'backwards', 'babysitter', 'babes', 'babe', 'babbling', 'azzz', 'ax', 'awwwww', 'awhile', 'awareness', 'awake', 'await', 'avoiding', 'avec', 'autograph', 'authorized', 'authority', 'auditions', 'auditioned', 'au', 'attract', 'attorney', 'attitude', 'attend', 'attacking', 'attacker', 'attacked', 'attach', 'atmosphere', 'astronomer', 'asteroid', 'assure', 'assuming', 'assign', 'asparagus', 'asbestos', 'artist', 'artifact', 'arthritic', 'artelle', 'art', 'arrogance', 'arrested', 'arrest', 'aroma', 'argument', 'arghh', 'argentinaaaa', 'architecture', 'architect', 'aquarius', 'aptitude', 'aprons', 'apron', 'approved', 'approaching', 'appreciation', 'applying', 'applied', 'applicants', 'apples', 'appealing', 'appeal', 'appalled', 'appalachia', 'apothecary', 'apologizing', 'apes', 'anxious', 'anvil', 'ants', 'antiquities', 'antiques', 'anti', 'anthropologists', 'anthropologically', 'answered', 'annulled', 'annoy', 'announces', 'announcement', 'anniversaries', 'annie', 'ankles', 'anguish', 'angle', 'angelica', 'angeles', 'andy', 'andrew', 'amusing', 'amount', 'americans', 'ambush', 'ambulance', 'ambitious', 'amazingly', 'amanda', 'alumni', 'altogether', 'alternative', 'alternate', 'allright', 'allow', 'allll', 'alley', 'allergic', 'allan', 'alfredo', 'alexandra', 'alex', 'alessandro', 'albino', 'al', 'airplane', 'airhead', 'agressive', 'aggressive', 'ageist', 'aged', 'agamemnon', 'afterwards', 'afternoons', 'affected', 'affect', 'aerobics', 'advised', 'adventure', 'advanced', 'advance', 'adorable', 'admitted', 'admits', 'admire', 'adjustment', 'adjusting', 'adjoining', 'adiós', 'addressed', 'add', 'acted', 'actally', 'acoustical', 'accounts', 'accountant', 'according', 'accidentally', 'access', 'academics', 'abysmal', 'aboard', 'abandoner', 'aaargh', 'aaaah', 'aa', '98', '94', '9323', '92', '9010', '87', '853', '79', '75', '7143457', '664', '60', '56', '555', '5000', '49', '47', '46', '437', '400', '37135', '350', '321', '29', '278', '26', '24', '23rd', '232', '21st', '2030', '1a', '1989', '1982', '1968', '1939', '1890', '16th', '16000', '1600', '16', '1500', '129', '1200', '12', '112', '110', '11']\n",
      "6426\n"
     ]
    }
   ],
   "source": [
    "train_text = list(dataset.map(lambda x, y: x).as_numpy_iterator())\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "print(len(vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation lines: tf.Tensor(b'<soc> <sol> Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked . <eol> ', shape=(), dtype=string)\n",
      "Emotion labels: tf.Tensor(b'neutral', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in dataset.take(1):\n",
    "    print(\"Conversation lines:\", text_batch)\n",
    "    print(\"Emotion labels:\", label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --->  sol\n",
      "3 --->  eol\n",
      "4 --->  i\n",
      "5 --->  you\n",
      "17 --->  soc\n"
     ]
    }
   ],
   "source": [
    "print(\"2 ---> \",vectorize_layer.get_vocabulary()[2])\n",
    "print(\"3 ---> \",vectorize_layer.get_vocabulary()[3])\n",
    "print(\"4 ---> \",vectorize_layer.get_vocabulary()[4])\n",
    "print(\"5 ---> \",vectorize_layer.get_vocabulary()[5])\n",
    "print(\"17 ---> \",vectorize_layer.get_vocabulary()[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line tf.Tensor(b'<soc> <sol> Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked . <eol> ', shape=(), dtype=string)\n",
      "Label tf.Tensor(b'neutral', shape=(), dtype=string)\n",
      "Vectorized line (<tf.Tensor: shape=(1, 30), dtype=int64, numpy=\n",
      "array([[  17,    2,  326,   27,    4,   20,  105,   33,  345,  410,    4,\n",
      "          20, 1088,   33,    7,  839,   31,    7, 2263,   13,    4,  654,\n",
      "           4,   20,  236,  323,    3,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=string, numpy=b'neutral'>)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "for text_batch, label_batch in dataset.take(1):\n",
    "    #text_batch, label_batch = text_batch[0], label_batch[0]\n",
    "    print(\"Line\", text_batch)\n",
    "    print(\"Label\", label_batch)\n",
    "    print(\"Vectorized line\", vectorize_text(text_batch, label_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 16)          96000     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,017\n",
      "Trainable params: 96,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string) Tensor(\"args_1:0\", shape=(), dtype=string)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Length for attr 'output_shapes' of 0 must be at least minimum 1\n\t; NodeDef: {{node MapDataset}}; Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=metadata:string,default=\"\"> [Op:MapDataset]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line, one_hot\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Map the encode_example function to the dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#encoded_dataset = dataset.map(lambda line, emotion: encode_example(line, emotion))\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line, label \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2048\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2045\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m   2046\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   2051\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2052\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2055\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2056\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5249\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m structured_function\u001b[38;5;241m.\u001b[39mStructuredFunctionWrapper(\n\u001b[1;32m   5244\u001b[0m     map_func,\n\u001b[1;32m   5245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformation_name(),\n\u001b[1;32m   5246\u001b[0m     dataset\u001b[38;5;241m=\u001b[39minput_dataset,\n\u001b[1;32m   5247\u001b[0m     use_legacy_function\u001b[38;5;241m=\u001b[39muse_legacy_function)\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m-> 5249\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5250\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   5251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5253\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5256\u001b[0m \u001b[38;5;28msuper\u001b[39m(MapDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3467\u001b[0m, in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3465\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3467\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3469\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Length for attr 'output_shapes' of 0 must be at least minimum 1\n\t; NodeDef: {{node MapDataset}}; Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=metadata:string,default=\"\"> [Op:MapDataset]"
     ]
    }
   ],
   "source": [
    "# Example labels\n",
    "labels = ['neutral', 'surprise', 'anger', 'sadness', 'joy', 'disgust', 'fear'] \n",
    "num_classes = 7\n",
    "\n",
    "label_to_index = {label: index for index, label in enumerate(labels)}\n",
    "\n",
    "# Function to encode a single example (line and emotion)\n",
    "def encode_example(line, emotion):\n",
    "    print(line)\n",
    "    print(emotion)\n",
    "    label = np.array(emotion).astype('str').decode(\"utf-8\")\n",
    "    index = label_to_index[label]\n",
    "    one_hot = tf.one_hot(index, len(labels))\n",
    "    return line, one_hot\n",
    "\n",
    "# Map the encode_example function to the dataset\n",
    "#encoded_dataset = dataset.map(lambda line, emotion: encode_example(line, emotion))\n",
    "\n",
    "dataset.map(lambda line, emotion: print(line, emotion))\n",
    "\n",
    "# Example usage\n",
    "for line, label in dataset.take(5):\n",
    "    print(\"Conversation line:\", line)\n",
    "    print(\" label:\", label)\n",
    "\n",
    "\n",
    "for train in train_ds.take(1):\n",
    "    print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 212, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/x1/dl1z_tcs7zb6pppfbf65d5sh0000gn/T/__autograph_generated_filen9xlhgb3.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/teodorastereciu/Documents/bachelors-project/mc-ecpe/.venv/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 212, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds.take(1000),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128, return_sequences=True)),\n",
    "    Attention(units=128),\n",
    "    tf.keras.layers.Dense(num_emotions, activation='sigmoid') \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "loss, accuracy = model.evaluate(val_data, val_labels)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc-ecpe-i06nAE3k-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
